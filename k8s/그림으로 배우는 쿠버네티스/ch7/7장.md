# 7장

파드의 심화 개념 — 레이블, 생명주기, 헬스체크, 멀티 컨테이너 패턴, 스케줄링 제어

```
                        Pod 심화
                           │
    ┌──────────┬───────────┼───────────┬──────────────┐
    ▼          ▼           ▼           ▼              ▼
  Label    생명주기     Health     멀티 컨테이너    스케줄링
 (분류/선택) (restart   Check      패턴            (affinity
             Policy)   (Probe)   (sidecar 등)     topology)
```

## 파드 레이블(label)

파드에 key=value 형태의 메타데이터를 부여하여 **분류, 선택, 필터링**에 활용

```
Pod-1                    Pod-2                    Pod-3
 labels:                  labels:                  labels:
  app: web                app: api                 app: web
  env: prod               env: prod                env: staging
  version: v2             version: v1              version: v1
  team: backend           team: backend            team: frontend
```

**레이블의 핵심 역할**
- Service가 트래픽을 전달할 Pod를 **선택**하는 기준
- Deployment/ReplicaSet이 관리할 Pod를 **식별**하는 기준
- kubectl 명령어로 리소스를 **필터링**하는 기준

```
Service (selector: app=web, env=prod)
         │
         ├──→ Pod-1 (app=web, env=prod)     ✅ 매칭
         ├──→ Pod-2 (app=api, env=prod)     ❌ app 불일치
         └──→ Pod-3 (app=web, env=staging)  ❌ env 불일치
```

**권장 레이블 (kubernetes.io 표준)**
| 레이블 | 설명 | 예시 |
| --- | --- | --- |
| `app.kubernetes.io/name` | 애플리케이션 이름 | `nginx` |
| `app.kubernetes.io/version` | 버전 | `v1.2.3` |
| `app.kubernetes.io/component` | 컴포넌트 역할 | `frontend` |
| `app.kubernetes.io/part-of` | 소속 시스템 | `online-shop` |
| `app.kubernetes.io/managed-by` | 관리 도구 | `helm` |

**레이블 관리**
```bash
# 레이블 추가
kubectl label pods my-pod app=web env=prod

# 레이블 확인
kubectl get pods --show-labels
kubectl get pods -L app,env

# 레이블 수정
kubectl label pods my-pod env=staging --overwrite

# 레이블 삭제
kubectl label pods my-pod env-
```

**셀렉터로 필터링**
```bash
# 일치 기반 (equality-based)
kubectl get pods -l app=web
kubectl get pods -l app=web,env=prod      # AND 조건
kubectl get pods -l app!=api              # 불일치

# 집합 기반 (set-based)
kubectl get pods -l 'env in (prod,staging)'
kubectl get pods -l 'env notin (dev)'
kubectl get pods -l 'gpu'                  # 키 존재 여부
kubectl get pods -l '!gpu'                 # 키 부재
```

**YAML에서 레이블 정의**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-pod
  labels:
    app: web
    env: prod
    version: v2
    team: backend
spec:
  containers:
    - name: nginx
      image: nginx:1.25
```

---

## 정적(Static) 파드

kubelet이 API 서버 없이 **직접 관리**하는 파드

```
일반 Pod:
  사용자 → API Server → Scheduler → kubelet → Pod 생성

Static Pod:
  kubelet이 지정 디렉토리 감시 → YAML 발견 → 직접 Pod 생성
  (API Server, Scheduler 관여 없음)
```

**동작 원리**
1. kubelet이 특정 디렉토리(기본: `/etc/kubernetes/manifests/`)를 주기적으로 감시
2. 해당 디렉토리에 YAML 파일이 있으면 자동으로 Pod 생성
3. YAML 수정 → Pod 자동 재생성, YAML 삭제 → Pod 자동 삭제
4. API 서버에 **미러 Pod(Mirror Pod)**가 읽기 전용으로 표시됨

```
/etc/kubernetes/manifests/
 ├─ etcd.yaml          → etcd Pod
 ├─ kube-apiserver.yaml → API Server Pod
 ├─ kube-controller-manager.yaml → Controller Manager Pod
 └─ kube-scheduler.yaml → Scheduler Pod

kubelet ──(감시)──→ 디렉토리 변경 감지 ──→ Pod 생성/삭제
                                              │
                                              ▼
                                     API Server에 Mirror Pod 등록
                                     (kubectl get pods로 보임)
```

**Static Pod의 특징**
| 항목 | 설명 |
| --- | --- |
| 관리 주체 | kubelet (API Server 아님) |
| 스케줄링 | 없음 — 해당 노드에서만 실행 |
| 삭제 방법 | YAML 파일 삭제 (kubectl delete 불가) |
| 네이밍 | `<pod-name>-<node-name>` 형식 |
| 컨트롤러 | ReplicaSet 등 사용 불가, kubelet이 직접 재시작 |

**kubelet 설정 확인**
```bash
# staticPodPath 확인
cat /var/lib/kubelet/config.yaml | grep staticPodPath
# 기본값: /etc/kubernetes/manifests

# 또는 kubelet 프로세스 옵션 확인
ps aux | grep kubelet | grep static
```

**Static Pod 만들기**
```bash
# 매니페스트 디렉토리에 YAML 파일 생성
cat <<EOF > /etc/kubernetes/manifests/static-nginx.yaml
apiVersion: v1
kind: Pod
metadata:
  name: static-nginx
spec:
  containers:
    - name: nginx
      image: nginx:1.25
      ports:
        - containerPort: 80
EOF

# 자동으로 생성된 Pod 확인 (Mirror Pod)
kubectl get pods
# NAME                    READY   STATUS    RESTARTS   AGE
# static-nginx-node-1     1/1     Running   0          10s

# kubectl delete로 삭제 시도 → 삭제되지만 즉시 재생성됨
kubectl delete pod static-nginx-node-1
# 다시 생성됨!

# 진짜 삭제하려면 YAML 파일 제거
rm /etc/kubernetes/manifests/static-nginx.yaml
```

**핵심 사용처: 컨트롤 플레인 컴포넌트**
- kubeadm으로 설치한 클러스터의 **컨트롤 플레인이 Static Pod로 실행**됨
  - etcd, kube-apiserver, kube-controller-manager, kube-scheduler
- API 서버가 아직 없는 부트스트랩 단계에서도 Pod를 실행할 수 있기 때문

---

## 컨테이너 상태에 따른 동작(restartPolicy)

Pod 내 컨테이너가 종료되었을 때 kubelet이 어떻게 대응할지 결정

```
Container 종료
      │
      ▼
restartPolicy 확인
      │
      ├─ Always     → 항상 재시작 (정상 종료든 에러든)
      ├─ OnFailure  → 에러(exit code ≠ 0)일 때만 재시작
      └─ Never      → 재시작 안함
```

**restartPolicy 종류**
| 정책 | 정상 종료 (exit 0) | 비정상 종료 (exit ≠ 0) | 사용 대상 |
| --- | --- | --- | --- |
| `Always` (기본값) | 재시작 | 재시작 | Deployment, Service 등 상시 실행 |
| `OnFailure` | 재시작 안함 | 재시작 | Job, 배치 작업 |
| `Never` | 재시작 안함 | 재시작 안함 | 일회성 디버깅, 로그 확인 |

**주의: restartPolicy는 Pod 레벨 설정 (컨테이너별 아님)**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: restart-example
spec:
  restartPolicy: OnFailure  # Pod 내 모든 컨테이너에 적용
  containers:
    - name: app
      image: busybox
      command: ["sh", "-c", "exit 1"]
```

**재시작 백오프(Backoff)**
```
1회 실패 → 즉시 재시작
2회 실패 → 10초 후 재시작
3회 실패 → 20초 후 재시작
4회 실패 → 40초 후 재시작
...
최대 5분(300초) 간격으로 재시작 반복

→ kubectl get pods에서 STATUS: CrashLoopBackOff
```

- 컨테이너가 반복적으로 실패하면 kubelet은 **지수 백오프(exponential backoff)**로 재시작 간격을 늘림
- 10초 → 20초 → 40초 → 80초 → 160초 → 300초(최대)
- 컨테이너가 10분 이상 정상 실행되면 백오프 타이머 리셋

**컨테이너 상태(containerStatuses)**
```
Waiting ──→ Running ──→ Terminated
   │            │             │
   │            └─ 정상 실행 중  └─ 종료됨 (exit code 확인)
   └─ 이미지 pull, 스케줄링 대기 등
```

| 상태 | 설명 |
| --- | --- |
| `Waiting` | 컨테이너 시작 전 (이미지 pull, 초기화 등) |
| `Running` | 컨테이너 실행 중 |
| `Terminated` | 컨테이너 종료됨 (정상/비정상) |

```bash
# 컨테이너 상태 확인
kubectl describe pod my-pod
# State:          Running / Waiting / Terminated
# Reason:         CrashLoopBackOff / ImagePullBackOff / ...
# Exit Code:      0 (정상) / 1, 137, 143 등 (비정상)

# 자주 보이는 Exit Code
# 0   → 정상 종료
# 1   → 일반 에러
# 137 → SIGKILL (OOMKilled 또는 강제 종료)
# 143 → SIGTERM (정상 종료 시그널)
```

**리소스별 restartPolicy 제약**
| 리소스 | 허용 restartPolicy |
| --- | --- |
| Deployment / ReplicaSet / DaemonSet | `Always`만 허용 |
| Job | `OnFailure` 또는 `Never` |
| CronJob | `OnFailure` 또는 `Never` |
| 단독 Pod | `Always`, `OnFailure`, `Never` 모두 가능 |

---

## 애플리케이션 상태 탐사(startupProbe, livenessProbe, readinessProbe)

kubelet이 컨테이너의 상태를 **주기적으로 점검**하여 적절한 조치를 수행

```
컨테이너 시작
      │
      ▼
 startupProbe ──(실패)──→ 컨테이너 재시작
      │ (성공)
      ▼
 ┌─────────────────────────────────────┐
 │  livenessProbe    readinessProbe    │
 │  (살아있나?)       (요청 받을 수 있나?) │
 │       │                 │           │
 │   실패 → 재시작     실패 → Service에서│
 │                     엔드포인트 제거   │
 └─────────────────────────────────────┘
```

**3가지 Probe 비교**
| Probe | 목적 | 실패 시 동작 | 시작 시점 |
| --- | --- | --- | --- |
| `startupProbe` | 앱 초기화 완료 확인 | 컨테이너 재시작 | 컨테이너 시작 직후 |
| `livenessProbe` | 앱이 살아있는지 확인 | 컨테이너 재시작 | startupProbe 성공 후 |
| `readinessProbe` | 트래픽 수신 가능한지 확인 | Service 엔드포인트 제거 | startupProbe 성공 후 |

**Probe 방식 (3가지)**

```
1. httpGet — HTTP 요청 → 200~399면 성공
   ┌──────────┐    GET /healthz    ┌──────────┐
   │ kubelet  │ ──────────────────→│ Container│
   └──────────┘    ← 200 OK       └──────────┘

2. tcpSocket — TCP 연결 시도 → 연결되면 성공
   ┌──────────┐    TCP SYN :3306   ┌──────────┐
   │ kubelet  │ ──────────────────→│ Container│
   └──────────┘    ← SYN-ACK      └──────────┘

3. exec — 컨테이너 내 명령 실행 → exit 0이면 성공
   ┌──────────┐  exec: cat /tmp/healthy  ┌──────────┐
   │ kubelet  │ ────────────────────────→│ Container│
   └──────────┘    ← exit 0             └──────────┘
```

**Probe 공통 파라미터**
| 파라미터 | 기본값 | 설명 |
| --- | --- | --- |
| `initialDelaySeconds` | 0 | 컨테이너 시작 후 첫 probe까지 대기 시간 |
| `periodSeconds` | 10 | probe 주기 |
| `timeoutSeconds` | 1 | probe 타임아웃 |
| `successThreshold` | 1 | 성공으로 간주할 연속 성공 횟수 |
| `failureThreshold` | 3 | 실패로 간주할 연속 실패 횟수 |

**startupProbe — 느린 앱 초기화 보호**

startupProbe가 성공하기 전까지 liveness/readinessProbe가 **비활성화**됨

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: slow-start-app
spec:
  containers:
    - name: app
      image: my-app:v1
      startupProbe:
        httpGet:
          path: /healthz
          port: 8080
        failureThreshold: 30   # 최대 30번 시도
        periodSeconds: 10       # 10초 간격
        # → 최대 300초(5분) 동안 초기화 대기
      livenessProbe:
        httpGet:
          path: /healthz
          port: 8080
        periodSeconds: 10
      readinessProbe:
        httpGet:
          path: /ready
          port: 8080
        periodSeconds: 5
```

- startupProbe가 없으면 livenessProbe의 `initialDelaySeconds`를 크게 잡아야 함
- 초기화 시간이 긴 Java/Spring 앱, 데이터 로딩이 필요한 ML 모델 서버 등에 필수

**livenessProbe — 데드락/행 감지**
```yaml
# HTTP 방식
livenessProbe:
  httpGet:
    path: /healthz
    port: 8080
    httpHeaders:
      - name: X-Custom-Header
        value: LivenessCheck
  initialDelaySeconds: 15
  periodSeconds: 20
  timeoutSeconds: 3
  failureThreshold: 3
```

```yaml
# exec 방식 (파일 존재 여부 확인)
livenessProbe:
  exec:
    command:
      - cat
      - /tmp/healthy
  periodSeconds: 5
```

```yaml
# TCP 방식 (DB 등 HTTP가 아닌 서비스)
livenessProbe:
  tcpSocket:
    port: 3306
  periodSeconds: 10
```

**readinessProbe — 트래픽 제어**
```yaml
readinessProbe:
  httpGet:
    path: /ready
    port: 8080
  initialDelaySeconds: 5
  periodSeconds: 5
  successThreshold: 1
  failureThreshold: 3
```

```
readinessProbe 실패 시:
Service ──→ Endpoints에서 해당 Pod IP 제거 ──→ 트래픽 전달 중단

readinessProbe 성공 시:
Service ──→ Endpoints에 Pod IP 다시 추가 ──→ 트래픽 전달 재개
```

**세 가지 Probe를 모두 사용하는 실전 예시**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: production-app
spec:
  containers:
    - name: app
      image: my-app:v1
      ports:
        - containerPort: 8080
      # 1. 초기화 대기 (최대 5분)
      startupProbe:
        httpGet:
          path: /healthz
          port: 8080
        failureThreshold: 30
        periodSeconds: 10
      # 2. 앱 프로세스가 살아있는지 (데드락 감지)
      livenessProbe:
        httpGet:
          path: /healthz
          port: 8080
        periodSeconds: 15
        timeoutSeconds: 3
        failureThreshold: 3
      # 3. 트래픽 수신 가능한지 (DB 연결, 캐시 로딩 등)
      readinessProbe:
        httpGet:
          path: /ready
          port: 8080
        periodSeconds: 5
        timeoutSeconds: 2
        failureThreshold: 3
```

**Spring Boot Actuator 연동**

`spring-boot-starter-actuator` 의존성을 추가하면 Kubernetes 환경을 자동 감지하여 프로브 엔드포인트를 제공

| 엔드포인트 | 용도 |
| --- | --- |
| `/actuator/health/liveness` | livenessProbe |
| `/actuator/health/readiness` | readinessProbe |

```yaml
livenessProbe:
  httpGet:
    path: /actuator/health/liveness
    port: 8080
readinessProbe:
  httpGet:
    path: /actuator/health/readiness
    port: 8080
```

---

## 초기화(init) 컨테이너

메인 컨테이너 실행 전에 **순서대로 실행되는 사전 작업용 컨테이너**

```
Pod 시작
   │
   ▼
Init Container 1 ──(완료)──→ Init Container 2 ──(완료)──→ Main Container
  (DB 마이그레이션)             (설정 파일 다운로드)           (앱 실행)
   │                           │
   └─ 실패 시 → restartPolicy에 따라 재시도 또는 Pod 실패
```

**특징**
| 항목 | Init 컨테이너 | 일반 컨테이너 |
| --- | --- | --- |
| 실행 순서 | 정의된 순서대로 **하나씩** | **동시에** 실행 |
| 완료 조건 | 반드시 **성공 종료(exit 0)** 필요 | 계속 실행 |
| Probe | 지원하지 않음 | liveness/readiness 지원 |
| 리소스 | 별도 requests/limits 설정 가능 | 별도 설정 |
| 재시작 | 실패 시 성공할 때까지 재시도 | restartPolicy에 따름 |

**기본 예시**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: init-demo
spec:
  initContainers:
    # 1번째: 의존 서비스 대기
    - name: wait-for-db
      image: busybox:1.36
      command: ['sh', '-c', 'until nslookup mysql-service; do echo waiting for mysql; sleep 2; done']

    # 2번째: 설정 파일 준비
    - name: init-config
      image: busybox:1.36
      command: ['sh', '-c', 'wget -O /config/app.conf http://config-server/app.conf']
      volumeMounts:
        - name: config-vol
          mountPath: /config

  containers:
    - name: app
      image: my-app:v1
      volumeMounts:
        - name: config-vol
          mountPath: /etc/app
          readOnly: true

  volumes:
    - name: config-vol
      emptyDir: {}
```

**Init 컨테이너의 리소스 계산**
```
Init 컨테이너 리소스는 메인 컨테이너와 별도로 계산됨

Pod의 effective resource request =
  max(모든 init 컨테이너 중 최대값, 모든 일반 컨테이너의 합)

예시:
  Init-1: cpu=500m, memory=256Mi
  Init-2: cpu=200m, memory=512Mi  ← init 중 memory 최대
  Main:   cpu=1000m, memory=128Mi

  Effective: cpu=1000m, memory=512Mi
```

**활용 패턴**

1. **의존 서비스 대기**
```yaml
initContainers:
  - name: wait-for-redis
    image: busybox:1.36
    command: ['sh', '-c', 'until nc -z redis-service 6379; do sleep 1; done']
```

2. **DB 마이그레이션/스키마 초기화**
```yaml
initContainers:
  - name: db-migrate
    image: my-app:v1
    command: ['python', 'manage.py', 'migrate']
    env:
      - name: DATABASE_URL
        valueFrom:
          secretKeyRef:
            name: db-secret
            key: url
```

3. **Git에서 소스/설정 클론**
```yaml
initContainers:
  - name: git-clone
    image: alpine/git
    command: ['git', 'clone', 'https://github.com/org/config.git', '/data']
    volumeMounts:
      - name: data
        mountPath: /data
```

4. **권한/파일시스템 준비**
```yaml
initContainers:
  - name: fix-permissions
    image: busybox:1.36
    command: ['sh', '-c', 'chown -R 1000:1000 /data']
    securityContext:
      runAsUser: 0
    volumeMounts:
      - name: data
        mountPath: /data
```

---

## 멀티 컨테이너 패턴(사이드카, 앰배서더, 어댑터)

하나의 Pod에 **역할이 다른 여러 컨테이너**를 함께 배치하는 설계 패턴

```
Pod 내 컨테이너들은 공유:
 ├─ 네트워크 (localhost로 통신, 같은 IP)
 ├─ 볼륨 (emptyDir 등으로 파일 공유)
 └─ 프로세스 네임스페이스 (선택적)
```

### 사이드카(Sidecar) 패턴

메인 컨테이너의 기능을 **보조/확장**하는 컨테이너

```
┌─────────────────── Pod ───────────────────┐
│                                           │
│  ┌─────────┐        ┌─────────────┐      │
│  │  Main   │──log──→│  Sidecar    │      │
│  │  App    │  파일   │  (Fluentd)  │──→ Elasticsearch
│  │         │        │  로그 수집    │      │
│  └─────────┘        └─────────────┘      │
│       │                    │              │
│       └────── Volume 공유 ─┘              │
└───────────────────────────────────────────┘
```

**대표 사용 사례**
- 로그 수집 (Fluentd, Filebeat)
- 서비스 메시 프록시 (Istio의 Envoy sidecar)
- 설정 파일 동기화 (git-sync)
- TLS 종료 (nginx sidecar)

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: sidecar-logging
spec:
  containers:
    # 메인 앱 — 로그를 파일로 출력
    - name: app
      image: my-app:v1
      volumeMounts:
        - name: log-vol
          mountPath: /var/log/app

    # 사이드카 — 로그 파일을 수집하여 외부로 전송
    - name: log-collector
      image: fluentd:v1.16
      volumeMounts:
        - name: log-vol
          mountPath: /var/log/app
          readOnly: true

  volumes:
    - name: log-vol
      emptyDir: {}
```

### 앰배서더(Ambassador) 패턴

메인 컨테이너 대신 **외부 연결을 대리(프록시)**하는 컨테이너

```
┌─────────────────── Pod ───────────────────┐
│                                           │
│  ┌─────────┐       ┌──────────────┐      │
│  │  Main   │──────→│  Ambassador  │──────┼──→ DB Primary (쓰기)
│  │  App    │ :6379  │  (Proxy)     │──────┼──→ DB Replica (읽기)
│  │         │       │  연결 라우팅   │      │
│  └─────────┘       └──────────────┘      │
│                                           │
│  App은 localhost:6379만 알면 됨            │
│  Ambassador가 읽기/쓰기 분리              │
└───────────────────────────────────────────┘
```

**대표 사용 사례**
- DB 읽기/쓰기 분리 프록시
- 로컬 캐시 프록시
- 서비스 디스커버리 프록시
- 외부 API 게이트웨이

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: ambassador-pattern
spec:
  containers:
    # 메인 앱 — localhost:6379로만 접속
    - name: app
      image: my-app:v1
      env:
        - name: REDIS_HOST
          value: "localhost"
        - name: REDIS_PORT
          value: "6379"

    # 앰배서더 — 실제 Redis 클러스터로 프록시
    - name: redis-proxy
      image: redis-proxy:v1
      ports:
        - containerPort: 6379
      env:
        - name: REDIS_PRIMARY
          value: "redis-primary.default.svc:6379"
        - name: REDIS_REPLICAS
          value: "redis-replicas.default.svc:6379"
```

### 어댑터(Adapter) 패턴

메인 컨테이너의 출력을 **표준 형식으로 변환**하는 컨테이너

```
┌─────────────────── Pod ───────────────────┐
│                                           │
│  ┌─────────┐       ┌──────────────┐      │
│  │  Main   │──────→│  Adapter     │──────┼──→ Prometheus
│  │  App    │ 독자   │  (Exporter)  │      │   (표준 메트릭)
│  │         │ 포맷   │  형식 변환    │      │
│  └─────────┘       └──────────────┘      │
│                                           │
│  App의 비표준 메트릭 → Prometheus 형식 변환 │
└───────────────────────────────────────────┘
```

**대표 사용 사례**
- 메트릭 형식 변환 (앱 고유 형식 → Prometheus exposition format)
- 로그 형식 표준화 (다양한 포맷 → JSON 구조화 로그)
- 프로토콜 변환

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: adapter-pattern
spec:
  containers:
    # 메인 앱 — 독자적인 메트릭 형식으로 출력
    - name: app
      image: legacy-app:v1
      volumeMounts:
        - name: metrics-vol
          mountPath: /var/metrics

    # 어댑터 — Prometheus 형식으로 변환하여 제공
    - name: metrics-adapter
      image: prom-adapter:v1
      ports:
        - containerPort: 9090
          name: metrics
      volumeMounts:
        - name: metrics-vol
          mountPath: /var/metrics
          readOnly: true

  volumes:
    - name: metrics-vol
      emptyDir: {}
```

### 패턴 비교 요약

| 패턴 | 방향 | 역할 | 예시 |
| --- | --- | --- | --- |
| Sidecar | 메인 **옆에서** 보조 | 기능 확장 | 로그 수집, 서비스 메시 |
| Ambassador | 메인 **앞에서** 대리 | 외부 연결 프록시 | DB 프록시, API 게이트웨이 |
| Adapter | 메인 **뒤에서** 변환 | 출력 형식 변환 | 메트릭 변환, 로그 표준화 |

**Kubernetes v1.28+ Sidecar Container (native)**
```yaml
# 기존에는 init 컨테이너 + restartPolicy로 사이드카를 구현했으나
# v1.28부터 native sidecar container 지원 (initContainers + restartPolicy: Always)
apiVersion: v1
kind: Pod
metadata:
  name: native-sidecar
spec:
  initContainers:
    - name: log-agent
      image: fluentd:v1.16
      restartPolicy: Always   # ← 이것이 native sidecar 핵심
      volumeMounts:
        - name: logs
          mountPath: /var/log
  containers:
    - name: app
      image: my-app:v1
      volumeMounts:
        - name: logs
          mountPath: /var/log
  volumes:
    - name: logs
      emptyDir: {}
```

- Native sidecar는 메인 컨테이너보다 **먼저 시작, 나중에 종료**됨
- Job에서 사이드카가 완료를 막는 문제 해결
- Istio 등 서비스 메시에서 적극 활용 중

---

## 파드 어피니티(affinity)와 안티어피니티(anti-affinity)

**Pod 간의 관계**를 기반으로 스케줄링을 제어 (6장의 Node Affinity와 다름!)

```
Node Affinity: Pod → Node 레이블 기준으로 배치
Pod Affinity:  Pod → 다른 Pod가 있는 위치 기준으로 배치

Pod Affinity (같이 배치):
  "cache Pod가 있는 노드에 web Pod도 배치해줘"

Pod Anti-Affinity (떨어뜨려 배치):
  "같은 app=web인 Pod끼리는 다른 노드에 배치해줘"
```

**Pod Affinity — 관련 Pod끼리 가까이 배치**

```
┌─── Node-1 ───┐   ┌─── Node-2 ───┐   ┌─── Node-3 ───┐
│ cache (Redis) │   │              │   │              │
│ web (nginx) ✅│   │              │   │              │
│               │   │              │   │              │
└───────────────┘   └──────────────┘   └──────────────┘
          ↑
  cache와 같은 노드에 web을 배치 → 네트워크 지연 최소화
```

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-with-cache
  labels:
    app: web
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
              - key: app
                operator: In
                values:
                  - cache
          topologyKey: kubernetes.io/hostname  # 같은 노드
  containers:
    - name: nginx
      image: nginx:1.25
```

**Pod Anti-Affinity — 같은 종류 Pod를 분산 배치**

```
┌─── Node-1 ───┐   ┌─── Node-2 ───┐   ┌─── Node-3 ───┐
│  web-1       │   │  web-2       │   │  web-3       │
│              │   │              │   │              │
└──────────────┘   └──────────────┘   └──────────────┘
       ↑                  ↑                  ↑
  app=web Pod끼리 서로 다른 노드에 배치 → 고가용성 확보
```

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      affinity:
        podAntiAffinity:
          # Hard: 반드시 다른 노드에 배치
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: web
              topologyKey: kubernetes.io/hostname
      containers:
        - name: nginx
          image: nginx:1.25
```

**Soft(선호) Anti-Affinity — 가능하면 분산, 불가능하면 허용**
```yaml
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app: web
          topologyKey: kubernetes.io/hostname
```

- 노드 수 < 레플리카 수일 때 hard anti-affinity를 쓰면 Pending 발생 → soft 사용

**topologyKey에 따른 분산 범위**
| topologyKey | 의미 | 활용 |
| --- | --- | --- |
| `kubernetes.io/hostname` | 노드 단위 분산 | 같은 노드에 동일 Pod 방지 |
| `topology.kubernetes.io/zone` | 가용영역 단위 분산 | AZ 장애 대비 HA |
| `topology.kubernetes.io/region` | 리전 단위 분산 | 멀티 리전 배치 |

**Affinity + Anti-Affinity 조합 예시: 웹-캐시 시스템**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 3
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      affinity:
        # cache Pod와 같은 zone에 배치 (가까이)
        podAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app: cache
                topologyKey: topology.kubernetes.io/zone
        # 같은 web Pod끼리는 다른 노드에 배치 (분산)
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app: web
              topologyKey: kubernetes.io/hostname
      containers:
        - name: nginx
          image: nginx:1.25
```

---

## 토폴로지 분배 제약 조건(topologySpreadConstraints)

Pod를 **토폴로지 도메인(zone, 노드 등)에 균등하게 분산**하는 기능

```
Pod Anti-Affinity는 "겹치지 않게"만 가능
topologySpreadConstraints는 "균등하게 분산" 가능

Anti-Affinity (불균형 가능):
  Zone-A: 3개 Pod     Zone-B: 0개 Pod    ← 이런 상황 방지 불가

topologySpreadConstraints (균등):
  Zone-A: 2개 Pod     Zone-B: 2개 Pod    ← 균등 분배 보장
```

**핵심 필드**
| 필드 | 설명 |
| --- | --- |
| `maxSkew` | 토폴로지 도메인 간 최대 허용 불균형 수 |
| `topologyKey` | 분산 기준 (hostname, zone 등) |
| `whenUnsatisfiable` | 조건 불만족 시 동작 (`DoNotSchedule` / `ScheduleAnyway`) |
| `labelSelector` | 분산 대상 Pod 선택 |
| `minDomains` | 최소 도메인 수 (v1.25+, beta) |

**maxSkew 이해**
```
maxSkew: 1이면 도메인 간 Pod 수 차이가 최대 1

Zone-A: 2개    Zone-B: 2개    → 차이 0 ✅
Zone-A: 3개    Zone-B: 2개    → 차이 1 ✅
Zone-A: 4개    Zone-B: 2개    → 차이 2 ❌ (DoNotSchedule이면 스케줄 거부)
```

**기본 예시 — Zone 균등 분산**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web
spec:
  replicas: 6
  selector:
    matchLabels:
      app: web
  template:
    metadata:
      labels:
        app: web
    spec:
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app: web
      containers:
        - name: nginx
          image: nginx:1.25
```

```
결과 (3개 zone, 6개 replica):
  Zone-A: 2개    Zone-B: 2개    Zone-C: 2개

maxSkew: 1이므로 최대 차이 1까지 허용:
  Zone-A: 3개    Zone-B: 2개    Zone-C: 2개  도 가능
```

**다중 제약 조건 — Zone + Node 균등 분산**
```yaml
topologySpreadConstraints:
  # 1. Zone 간 균등
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app: web
  # 2. Node 간 균등
  - maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: ScheduleAnyway   # 노드 수 부족하면 허용
    labelSelector:
      matchLabels:
        app: web
```

```
결과 (2 zone × 3 node):
Zone-A                           Zone-B
├── Node-1: web-1               ├── Node-4: web-4
├── Node-2: web-2               ├── Node-5: web-5
└── Node-3: web-3               └── Node-6: web-6
   Zone 간 3:3 균등, Node 간 1:1:1 균등
```

**whenUnsatisfiable 비교**
| 값 | 설명 |
| --- | --- |
| `DoNotSchedule` | 조건 불만족 시 스케줄 거부 (Pending) — Hard |
| `ScheduleAnyway` | 조건 불만족이어도 가능한 균등하게 배치 — Soft |

**matchLabelKeys (v1.27+)**
```yaml
topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: DoNotSchedule
    labelSelector:
      matchLabels:
        app: web
    matchLabelKeys:
      - pod-template-hash   # 같은 revision의 Pod만 대상
```

- 롤링 업데이트 중 **신규 revision Pod만** 균등 분산 대상으로 지정 가능
- 기존 revision Pod와 섞이지 않아 더 정확한 분산

**Pod Anti-Affinity vs topologySpreadConstraints**
| 비교 항목 | Pod Anti-Affinity | topologySpreadConstraints |
| --- | --- | --- |
| 분산 방식 | "겹치지 않게" | "균등하게" |
| 제어 수준 | 있다/없다 이진 | maxSkew로 세밀 조절 |
| 다중 토폴로지 | topologyKey 하나씩 | 여러 제약 조건 동시 적용 |
| 유연성 | 낮음 | 높음 |
| 성능 영향 | 크지 않음 | 대규모 클러스터에서 주의 |
| 적합한 상황 | "같은 노드에 중복 배치 금지" | "AZ/노드에 고르게 분산" |

**클러스터 기본값 설정 (v1.24+)**
```yaml
# kube-scheduler 설정 (KubeSchedulerConfiguration)
apiVersion: kubescheduler.config.k8s.io/v1
kind: KubeSchedulerConfiguration
profiles:
  - pluginConfig:
      - name: PodTopologySpread
        args:
          defaultConstraints:
            - maxSkew: 1
              topologyKey: topology.kubernetes.io/zone
              whenUnsatisfiable: ScheduleAnyway
          defaultingType: List
```

- 클러스터 전체에 기본 분산 정책을 적용 → 개별 Deployment마다 설정할 필요 없음

