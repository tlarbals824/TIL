# 8장

클러스터 보안, 자원 관리, 네트워크 격리 — RBAC, ResourceQuota, LimitRange, NetworkPolicy

```
                     클러스터 보안 & 관리
                           │
    ┌──────────┬───────────┼───────────────┐
    ▼          ▼           ▼               ▼
  RBAC     ResourceQuota  LimitRange    NetworkPolicy
 (누가 무엇을  (네임스페이스   (Pod/컨테이너   (Pod 간
  할 수 있나)   총량 제한)     기본값/범위)    통신 제어)
```

---

## 역할 기반 접근 제어(RBAC)

**누가(Subject)**, **무엇을(Resource)**, **어떻게(Verb)** 할 수 있는지를 제어하는 인가(Authorization) 메커니즘

```
사용자/ServiceAccount
        │
        ▼
  인증(Authentication) ──→ "너는 누구인가?"
        │
        ▼
  인가(Authorization)  ──→ "너는 이걸 할 수 있는가?" ← RBAC
        │
        ▼
  Admission Control    ──→ "요청 내용이 정책에 맞는가?"
        │
        ▼
    API Server 처리
```

### RBAC 핵심 구성요소

```
Subject (누가)              Role (무엇을, 어떻게)           Binding (연결)
 ├─ User                    ├─ Role (네임스페이스 범위)       ├─ RoleBinding
 ├─ Group                   └─ ClusterRole (클러스터 범위)    └─ ClusterRoleBinding
 └─ ServiceAccount

Subject ──(Binding)──→ Role
  "dev-user를 pod-reader Role에 연결"
```

| 구성요소 | 범위 | 설명 |
| --- | --- | --- |
| `Role` | 네임스페이스 | 특정 네임스페이스 내 권한 정의 |
| `ClusterRole` | 클러스터 전체 | 클러스터 범위 권한 정의 (노드, PV 등 포함) |
| `RoleBinding` | 네임스페이스 | Subject를 Role에 연결 |
| `ClusterRoleBinding` | 클러스터 전체 | Subject를 ClusterRole에 연결 |

### Role — 네임스페이스 범위 권한

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: dev
  name: pod-reader
rules:
  - apiGroups: [""]          # core API 그룹 (Pod, Service 등)
    resources: ["pods"]
    verbs: ["get", "watch", "list"]
  - apiGroups: [""]
    resources: ["pods/log"]   # 하위 리소스
    verbs: ["get"]
```

**주요 Verb(동작)**
| Verb | 설명 | kubectl 매핑 |
| --- | --- | --- |
| `get` | 개별 리소스 조회 | `kubectl get pod my-pod` |
| `list` | 리소스 목록 조회 | `kubectl get pods` |
| `watch` | 변경 감시 | `kubectl get pods -w` |
| `create` | 리소스 생성 | `kubectl create` / `kubectl apply` |
| `update` | 리소스 수정 | `kubectl apply` / `kubectl edit` |
| `patch` | 리소스 부분 수정 | `kubectl patch` |
| `delete` | 리소스 삭제 | `kubectl delete` |
| `deletecollection` | 리소스 일괄 삭제 | `kubectl delete pods --all` |

**주요 API 그룹**
| apiGroups | 포함 리소스 |
| --- | --- |
| `""` (core) | pods, services, configmaps, secrets, persistentvolumeclaims |
| `apps` | deployments, replicasets, statefulsets, daemonsets |
| `batch` | jobs, cronjobs |
| `networking.k8s.io` | networkpolicies, ingresses |
| `rbac.authorization.k8s.io` | roles, clusterroles, rolebindings |

### ClusterRole — 클러스터 범위 권한

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: node-reader
rules:
  - apiGroups: [""]
    resources: ["nodes"]       # 노드는 네임스페이스 없음 → ClusterRole 필요
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["persistentvolumes"]
    verbs: ["get", "list"]
```

**ClusterRole이 필요한 경우**
- 네임스페이스 없는 리소스: nodes, persistentvolumes, namespaces
- 모든 네임스페이스에 동일한 권한 적용 시 (ClusterRole + RoleBinding 조합)
- 비 리소스 URL 접근: `/healthz`, `/metrics`

### RoleBinding — Subject와 Role 연결

```
RoleBinding: "dev-user에게 dev 네임스페이스의 pod-reader Role 부여"

dev-user ──(RoleBinding)──→ Role: pod-reader
                                  │
                                  ▼
                            dev 네임스페이스에서
                            pods get/list/watch 가능
```

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: dev
subjects:
  - kind: User
    name: dev-user
    apiGroup: rbac.authorization.k8s.io
  - kind: ServiceAccount
    name: my-app
    namespace: dev
roleRef:
  kind: Role
  name: pod-reader
  apiGroup: rbac.authorization.k8s.io
```

### ClusterRoleBinding — 클러스터 전체에 권한 부여

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-binding
subjects:
  - kind: Group
    name: platform-team
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-admin       # 기본 제공 ClusterRole
  apiGroup: rbac.authorization.k8s.io
```

### Binding 조합 패턴

```
① Role + RoleBinding
   → 특정 네임스페이스에서만 권한 부여

② ClusterRole + ClusterRoleBinding
   → 클러스터 전체에서 권한 부여

③ ClusterRole + RoleBinding
   → ClusterRole을 특정 네임스페이스에만 적용
   → 재사용성 높음 (하나의 ClusterRole을 여러 네임스페이스에 바인딩)
```

| 조합 | 범위 | 활용 |
| --- | --- | --- |
| Role + RoleBinding | 단일 네임스페이스 | 팀별 권한 격리 |
| ClusterRole + ClusterRoleBinding | 클러스터 전체 | 관리자, 모니터링 |
| ClusterRole + RoleBinding | 특정 네임스페이스 | 공통 Role 재사용 |

### ServiceAccount

Pod가 API 서버에 접근할 때 사용하는 **자동 인증 주체**

```
Pod 생성 시:
  1. ServiceAccount 지정 (기본: default)
  2. 해당 SA의 토큰이 Pod에 자동 마운트
  3. Pod 내 프로세스가 토큰으로 API 서버 인증

Pod ──(SA 토큰)──→ API Server ──(RBAC 확인)──→ 허용/거부
```

```yaml
# ServiceAccount 생성
apiVersion: v1
kind: ServiceAccount
metadata:
  name: my-app
  namespace: dev
---
# Pod에서 ServiceAccount 사용
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  namespace: dev
spec:
  serviceAccountName: my-app     # 이 SA의 권한으로 API 접근
  automountServiceAccountToken: true
  containers:
    - name: app
      image: my-app:v1
```

```bash
# ServiceAccount 관리
kubectl create serviceaccount my-app -n dev
kubectl get serviceaccounts -n dev

# 토큰 마운트 경로 (Pod 내부)
# /var/run/secrets/kubernetes.io/serviceaccount/token
```

### 기본 제공 ClusterRole

| ClusterRole | 설명 |
| --- | --- |
| `cluster-admin` | 모든 리소스에 대한 모든 권한 |
| `admin` | 네임스페이스 내 대부분 리소스 관리 (RBAC 제외) |
| `edit` | 읽기/쓰기 가능 (Role/Binding 제외) |
| `view` | 읽기 전용 (Secret도 조회 가능) |

```bash
# 기본 ClusterRole 확인
kubectl get clusterroles | grep -E '^(cluster-admin|admin|edit|view)\s'

# 특정 Role의 상세 권한 확인
kubectl describe clusterrole view
```

### 권한 확인 (kubectl auth can-i)

```bash
# 현재 사용자의 권한 확인
kubectl auth can-i create deployments
kubectl auth can-i delete pods --namespace dev

# 특정 사용자의 권한 확인 (관리자)
kubectl auth can-i get pods --as dev-user --namespace dev

# ServiceAccount의 권한 확인
kubectl auth can-i list secrets --as system:serviceaccount:dev:my-app

# 현재 사용자의 모든 권한 나열
kubectl auth can-i --list --namespace dev
```

### 실전 예시: 팀별 네임스페이스 격리

```yaml
# 1. 네임스페이스 생성
apiVersion: v1
kind: Namespace
metadata:
  name: team-a
---
# 2. 팀 전용 Role (Pod, Deployment 관리 가능)
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: team-a
  name: team-a-dev
rules:
  - apiGroups: [""]
    resources: ["pods", "services", "configmaps"]
    verbs: ["get", "list", "watch", "create", "update", "delete"]
  - apiGroups: ["apps"]
    resources: ["deployments", "replicasets"]
    verbs: ["get", "list", "watch", "create", "update", "delete"]
  - apiGroups: [""]
    resources: ["pods/log", "pods/exec"]
    verbs: ["get", "create"]
---
# 3. RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  namespace: team-a
  name: team-a-dev-binding
subjects:
  - kind: Group
    name: team-a-developers
    apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: team-a-dev
  apiGroup: rbac.authorization.k8s.io
```

---

## 시스템 자원 사용 관리

```
자원 관리 메커니즘:

네임스페이스 레벨:
  ResourceQuota ──→ 네임스페이스 전체의 자원 총량 제한
                    "이 네임스페이스는 CPU 10코어, 메모리 20Gi까지만"

Pod/컨테이너 레벨:
  LimitRange ──→ 개별 Pod/컨테이너의 자원 기본값/범위 설정
                 "컨테이너당 CPU 최소 100m, 최대 2코어"

컨테이너 spec:
  resources.requests ──→ 스케줄링 기준 (보장량)
  resources.limits   ──→ 최대 사용량 (초과 시 제한/OOMKill)
```

### 리소스 요청(requests)과 제한(limits)

```
resources:
  requests:           limits:
    cpu: 250m           cpu: 500m
    memory: 128Mi       memory: 256Mi

     requests              limits
        │                     │
        ▼                     ▼
  "최소 이만큼 보장해줘"   "최대 이만큼만 허용해"
  (스케줄러가 노드 선택    (초과 시 CPU: 스로틀링
   시 사용)                       메모리: OOMKill)
```

```
CPU 단위:
  1 = 1 vCPU (1000m)
  500m = 0.5 vCPU
  100m = 0.1 vCPU

Memory 단위:
  Ki (1024), Mi (1024²), Gi (1024³)
  128Mi = 128 × 1024 × 1024 bytes
```

**requests vs limits 동작**
| 리소스 | requests 미달 시 | limits 초과 시 |
| --- | --- | --- |
| CPU | 스케줄 안됨 (Pending) | **스로틀링** (느려짐, 죽지 않음) |
| Memory | 스케줄 안됨 (Pending) | **OOMKill** (컨테이너 종료) |

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: resource-demo
spec:
  containers:
    - name: app
      image: my-app:v1
      resources:
        requests:
          cpu: "250m"
          memory: "128Mi"
        limits:
          cpu: "500m"
          memory: "256Mi"
```

### QoS(Quality of Service) 클래스

```
requests/limits 설정에 따라 Pod의 QoS 클래스가 자동 결정됨

  Guaranteed     >     Burstable     >     BestEffort
 (최우선 보장)       (부분 보장)          (보장 없음)
                                         ← OOMKill 우선순위 (먼저 죽음)
```

| QoS 클래스 | 조건 | OOMKill 순서 |
| --- | --- | --- |
| `Guaranteed` | 모든 컨테이너에 requests = limits 설정 | 가장 마지막 |
| `Burstable` | requests와 limits가 다르거나 일부만 설정 | 중간 |
| `BestEffort` | requests/limits 둘 다 미설정 | 가장 먼저 |

```yaml
# Guaranteed (requests == limits)
resources:
  requests:
    cpu: "500m"
    memory: "256Mi"
  limits:
    cpu: "500m"
    memory: "256Mi"

# Burstable (requests < limits)
resources:
  requests:
    cpu: "250m"
    memory: "128Mi"
  limits:
    cpu: "500m"
    memory: "256Mi"

# BestEffort (미설정)
# resources: 없음
```

### 리소스 쿼터 / ResourceQuota

**네임스페이스 전체의 자원 사용량 총합**을 제한

```
Namespace: dev
  ResourceQuota: dev-quota
    CPU 총합 ≤ 10코어
    Memory 총합 ≤ 20Gi
    Pod 수 ≤ 20개

    현재 사용:
     Pod-1: cpu 500m, mem 1Gi  ┐
     Pod-2: cpu 1,   mem 2Gi   ├─ 합계: cpu 2.5, mem 5Gi, 4 Pods
     Pod-3: cpu 500m, mem 1Gi  │
     Pod-4: cpu 500m, mem 1Gi  ┘

    → 새 Pod 생성 시 총합이 Quota를 넘으면 거부
```

**컴퓨트 리소스 쿼터**
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    requests.cpu: "10"            # 네임스페이스 CPU request 총합 ≤ 10 코어
    requests.memory: "20Gi"       # 네임스페이스 Memory request 총합 ≤ 20Gi
    limits.cpu: "20"              # 네임스페이스 CPU limit 총합 ≤ 20 코어
    limits.memory: "40Gi"         # 네임스페이스 Memory limit 총합 ≤ 40Gi
```

**오브젝트 수 쿼터**
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: object-quota
  namespace: dev
spec:
  hard:
    pods: "20"                    # Pod 최대 20개
    services: "10"                # Service 최대 10개
    services.loadbalancers: "2"   # LoadBalancer 최대 2개
    services.nodeports: "5"       # NodePort 최대 5개
    configmaps: "20"              # ConfigMap 최대 20개
    secrets: "20"                 # Secret 최대 20개
    persistentvolumeclaims: "10"  # PVC 최대 10개
```

**스토리지 쿼터**
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage-quota
  namespace: dev
spec:
  hard:
    requests.storage: "100Gi"                          # 총 스토리지 요청량
    persistentvolumeclaims: "10"                       # PVC 수
    fast-storage.storageclass.storage.k8s.io/requests.storage: "50Gi"  # 특정 StorageClass
    fast-storage.storageclass.storage.k8s.io/persistentvolumeclaims: "5"
```

**Scope로 쿼터 대상 제한**
```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort-quota
  namespace: dev
spec:
  hard:
    pods: "5"
  scopes:
    - BestEffort       # BestEffort QoS Pod에만 적용
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: not-besteffort-quota
  namespace: dev
spec:
  hard:
    pods: "20"
    requests.cpu: "10"
    requests.memory: "20Gi"
  scopes:
    - NotBestEffort    # Guaranteed/Burstable Pod에만 적용
```

| Scope | 대상 |
| --- | --- |
| `BestEffort` | QoS가 BestEffort인 Pod |
| `NotBestEffort` | QoS가 Guaranteed/Burstable인 Pod |
| `Terminating` | `activeDeadlineSeconds`가 설정된 Pod |
| `NotTerminating` | `activeDeadlineSeconds`가 미설정인 Pod |

```bash
# ResourceQuota 확인
kubectl get resourcequota -n dev
kubectl describe resourcequota compute-quota -n dev
# → Used / Hard 컬럼으로 현재 사용량 대비 한도 확인
```

**주의사항**
- ResourceQuota가 설정된 네임스페이스에서는 **Pod에 반드시 requests/limits를 지정**해야 함
  - 미지정 시 Pod 생성이 거부됨
  - → LimitRange와 함께 사용하여 기본값 설정 권장

### 리밋 레인지 / LimitRange

**개별 Pod/컨테이너**의 리소스 기본값, 최솟값, 최댓값을 설정

```
ResourceQuota: 네임스페이스 전체 총량 제한 (거시적)
LimitRange:    개별 컨테이너/Pod 범위 설정 (미시적)

LimitRange:
  default: cpu=500m, mem=256Mi    ← 미지정 시 자동 부여
  min: cpu=100m, mem=64Mi         ← 이 이하로 설정 불가
  max: cpu=2, mem=2Gi             ← 이 이상으로 설정 불가

Pod 생성 시:
  resources 미지정 → default 값 자동 적용
  min 미만 설정   → 거부
  max 초과 설정   → 거부
```

**컨테이너 레벨 LimitRange**
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: container-limits
  namespace: dev
spec:
  limits:
    - type: Container
      default:              # limits 기본값 (미지정 시 적용)
        cpu: "500m"
        memory: "256Mi"
      defaultRequest:       # requests 기본값 (미지정 시 적용)
        cpu: "200m"
        memory: "128Mi"
      min:                  # 최솟값
        cpu: "100m"
        memory: "64Mi"
      max:                  # 최댓값
        cpu: "2"
        memory: "2Gi"
      maxLimitRequestRatio: # limits/requests 비율 상한
        cpu: "4"            # limits가 requests의 최대 4배
        memory: "4"
```

**Pod 레벨 LimitRange**
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: pod-limits
  namespace: dev
spec:
  limits:
    - type: Pod
      min:
        cpu: "200m"
        memory: "128Mi"
      max:                  # Pod 내 모든 컨테이너 합계 기준
        cpu: "4"
        memory: "4Gi"
```

**PVC 레벨 LimitRange**
```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: storage-limits
  namespace: dev
spec:
  limits:
    - type: PersistentVolumeClaim
      min:
        storage: "1Gi"
      max:
        storage: "50Gi"
```

**LimitRange 동작 요약**
| 필드 | 설명 | 적용 시점 |
| --- | --- | --- |
| `default` | limits 기본값 | Pod 생성 시 미지정이면 자동 적용 |
| `defaultRequest` | requests 기본값 | Pod 생성 시 미지정이면 자동 적용 |
| `min` | 최솟값 | 이 이하면 생성 거부 |
| `max` | 최댓값 | 이 이상이면 생성 거부 |
| `maxLimitRequestRatio` | limits/requests 비율 상한 | 비율 초과 시 거부 |

```bash
# LimitRange 확인
kubectl get limitrange -n dev
kubectl describe limitrange container-limits -n dev
```

### ResourceQuota + LimitRange 조합 예시

```yaml
# 1. LimitRange — 개별 기본값/범위 설정
apiVersion: v1
kind: LimitRange
metadata:
  name: default-limits
  namespace: dev
spec:
  limits:
    - type: Container
      default:
        cpu: "500m"
        memory: "256Mi"
      defaultRequest:
        cpu: "200m"
        memory: "128Mi"
      max:
        cpu: "2"
        memory: "2Gi"
---
# 2. ResourceQuota — 네임스페이스 총량 제한
apiVersion: v1
kind: ResourceQuota
metadata:
  name: dev-quota
  namespace: dev
spec:
  hard:
    requests.cpu: "10"
    requests.memory: "20Gi"
    limits.cpu: "20"
    limits.memory: "40Gi"
    pods: "30"
```

```
Pod 생성 흐름:

1. Pod에 resources 미지정?
   → LimitRange의 default/defaultRequest 자동 적용

2. resources가 LimitRange min~max 범위 내?
   → 아니면 거부

3. 네임스페이스 총 사용량 + 이 Pod ≤ ResourceQuota?
   → 초과하면 거부

4. 모든 조건 통과 → Pod 생성 성공
```

---

## 네트워크 정책(NetworkPolicy)

Pod 간 네트워크 트래픽을 **허용/차단**하는 방화벽 규칙

```
기본 동작:
  NetworkPolicy 없음 → 모든 Pod 간 통신 허용 (default allow all)

NetworkPolicy 적용 후:
  해당 Pod에 대해 명시적으로 허용된 트래픽만 통과
  (정책에 매칭되지 않는 트래픽은 차단)
```

```
┌─────────── Namespace: production ───────────┐
│                                             │
│  ┌─────────┐    ┌─────────┐   ┌─────────┐ │
│  │  web     │───→│  api    │──→│  db     │ │
│  │  (front) │    │  (back) │   │ (mysql) │ │
│  └─────────┘    └─────────┘   └─────────┘ │
│       ↑              ↑             ↑       │
│   외부 허용       web만 허용    api만 허용   │
│   (Ingress)                                │
└─────────────────────────────────────────────┘

NetworkPolicy로:
  db는 api로부터만 트래픽 수신
  api는 web으로부터만 트래픽 수신
  web은 외부로부터 트래픽 수신 가능
```

**전제 조건: CNI 플러그인이 NetworkPolicy를 지원해야 함**
| CNI 플러그인 | NetworkPolicy 지원 |
| --- | --- |
| Calico | O |
| Cilium | O |
| Weave Net | O |
| Flannel | **X** (단독 사용 시) |
| AWS VPC CNI | O (Calico 추가 시) |

### NetworkPolicy 구조

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-policy
  namespace: production
spec:
  podSelector:          # 정책 대상 Pod 선택
    matchLabels:
      app: api
  policyTypes:          # 정책 방향
    - Ingress           # 들어오는 트래픽
    - Egress            # 나가는 트래픽
  ingress:              # 수신 허용 규칙
    - from:
        - podSelector:
            matchLabels:
              app: web
      ports:
        - protocol: TCP
          port: 8080
  egress:               # 송신 허용 규칙
    - to:
        - podSelector:
            matchLabels:
              app: db
      ports:
        - protocol: TCP
          port: 3306
```

```
위 정책의 의미:

app=api Pod에 대해:
  Ingress: app=web Pod에서 TCP 8080으로 들어오는 트래픽만 허용
  Egress:  app=db Pod의 TCP 3306으로 나가는 트래픽만 허용
  그 외 모든 트래픽은 차단
```

### 핵심 필드

| 필드 | 설명 |
| --- | --- |
| `podSelector` | 정책 적용 대상 Pod (빈 `{}`이면 네임스페이스 내 전체 Pod) |
| `policyTypes` | `Ingress`, `Egress`, 또는 둘 다 |
| `ingress[].from` | 수신 허용 출발지 |
| `egress[].to` | 송신 허용 목적지 |
| `ports` | 허용 포트/프로토콜 |

### from/to 셀렉터 종류

```yaml
# 1. podSelector — 같은 네임스페이스의 Pod
from:
  - podSelector:
      matchLabels:
        app: web

# 2. namespaceSelector — 특정 네임스페이스의 모든 Pod
from:
  - namespaceSelector:
      matchLabels:
        env: production

# 3. podSelector + namespaceSelector — 특정 네임스페이스의 특정 Pod
from:
  - podSelector:
      matchLabels:
        app: monitoring
    namespaceSelector:
      matchLabels:
        team: ops

# 4. ipBlock — CIDR 기반 IP 범위
from:
  - ipBlock:
      cidr: 10.0.0.0/8
      except:
        - 10.0.1.0/24
```

**주의: AND vs OR 조건**
```yaml
# OR 조건 (배열의 별도 항목) — 둘 중 하나라도 매칭되면 허용
from:
  - podSelector:          # 항목 1: app=web인 Pod
      matchLabels:
        app: web
  - namespaceSelector:    # 항목 2: env=prod인 네임스페이스의 모든 Pod
      matchLabels:
        env: prod

# AND 조건 (같은 항목에 함께) — 둘 다 매칭되어야 허용
from:
  - podSelector:          # env=prod 네임스페이스의 app=web Pod만
      matchLabels:
        app: web
    namespaceSelector:
      matchLabels:
        env: prod
```

### 기본 정책 패턴

**모든 수신 트래픽 차단 (Default Deny Ingress)**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: production
spec:
  podSelector: {}         # 네임스페이스 내 모든 Pod
  policyTypes:
    - Ingress
  # ingress 규칙 없음 → 모든 수신 차단
```

**모든 송신 트래픽 차단 (Default Deny Egress)**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-egress
  namespace: production
spec:
  podSelector: {}
  policyTypes:
    - Egress
  # egress 규칙 없음 → 모든 송신 차단
```

**모든 수신/송신 트래픽 차단 (Default Deny All)**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
```

**모든 트래픽 허용 (Default Allow All)**
```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - {}                  # 모든 수신 허용
  egress:
    - {}                  # 모든 송신 허용
```

### 실전 예시: 3계층 아키텍처 네트워크 정책

```
Internet ──→ web (frontend) ──→ api (backend) ──→ db (database)
              ↕ DNS                ↕ DNS              ↕ DNS
```

```yaml
# 1. Default Deny — 기본적으로 모든 트래픽 차단
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
  namespace: production
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
---
# 2. DNS 허용 — 모든 Pod가 DNS 조회 가능하도록
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-dns
  namespace: production
spec:
  podSelector: {}
  policyTypes:
    - Egress
  egress:
    - to: []
      ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
---
# 3. Web — 외부 수신 허용 + API로 송신 허용
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: web-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: web
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - ports:
        - protocol: TCP
          port: 80
  egress:
    - to:
        - podSelector:
            matchLabels:
              app: api
      ports:
        - protocol: TCP
          port: 8080
---
# 4. API — Web에서만 수신 + DB로만 송신
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: api-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: api
  policyTypes:
    - Ingress
    - Egress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: web
      ports:
        - protocol: TCP
          port: 8080
  egress:
    - to:
        - podSelector:
            matchLabels:
              app: db
      ports:
        - protocol: TCP
          port: 3306
---
# 5. DB — API에서만 수신, 송신 불필요
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: db
  policyTypes:
    - Ingress
  ingress:
    - from:
        - podSelector:
            matchLabels:
              app: api
      ports:
        - protocol: TCP
          port: 3306
```

### 네임스페이스 간 통신 제어

```yaml
# monitoring 네임스페이스의 prometheus가 모든 네임스페이스의 Pod에 접근 허용
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-monitoring
  namespace: production
spec:
  podSelector: {}
  policyTypes:
    - Ingress
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              purpose: monitoring
          podSelector:
            matchLabels:
              app: prometheus
      ports:
        - protocol: TCP
          port: 9090
```

```bash
# 네임스페이스에 레이블 추가 (namespaceSelector용)
kubectl label namespace monitoring purpose=monitoring
```

### NetworkPolicy 디버깅

```bash
# 정책 목록 확인
kubectl get networkpolicy -n production

# 정책 상세 확인
kubectl describe networkpolicy api-policy -n production

# Pod에 적용된 정책 확인 (Pod 레이블 기준)
kubectl get networkpolicy -n production -o wide

# 통신 테스트 (임시 Pod에서)
kubectl run test-pod --rm -it --image=busybox --restart=Never -n production -- wget -qO- --timeout=3 http://api-service:8080/health

# 특정 Pod에서 연결 테스트
kubectl exec -it web-pod -n production -- curl -m 3 http://api-service:8080
```

### NetworkPolicy 동작 규칙 요약

| 상황 | 결과 |
| --- | --- |
| NetworkPolicy 없음 | 모든 트래픽 허용 |
| podSelector에 매칭되는 정책 존재 + ingress 규칙 없음 | 해당 Pod로의 모든 수신 차단 |
| podSelector에 매칭되는 정책 존재 + egress 규칙 없음 | 해당 Pod의 모든 송신 차단 |
| 여러 NetworkPolicy가 같은 Pod에 적용 | **합집합(Union)** — 하나라도 허용하면 허용 |
| `ingress: [{}]` | 모든 수신 허용 |
| `from: [{}]` | 모든 출발지 허용 |
