# 9장

애플리케이션 배포·운영 — ConfigMap, Secret, 롤링 업데이트, Kustomize, Helm, Metrics Server, HPA, Dashboard

```
                    애플리케이션 배포 & 운영
                           │
    ┌──────────┬───────────┼───────────┬──────────────┐
    ▼          ▼           ▼           ▼              ▼
 ConfigMap   Secret     Rolling     패키징/배포     모니터링/스케일링
 (설정 주입) (암호화    Update     (Kustomize,    (Metrics Server,
             설정 주입)  (set,       Helm)         HPA, Dashboard)
                       rollout)
```

---

## 애플리케이션에 컨피그맵(ConfigMap)을 이용해 추가 설정 넣기

**컨테이너 이미지와 설정을 분리**하여 환경별로 다른 설정을 주입하는 메커니즘

```
기존 문제:
  설정이 이미지에 포함 → 환경(dev/staging/prod)마다 이미지를 따로 빌드해야 함

ConfigMap 해결:
  이미지는 하나 + ConfigMap으로 환경별 설정만 교체

  ┌──────────┐     ┌──────────────┐
  │  동일한   │ ←── │ ConfigMap    │
  │  이미지   │     │ (dev 설정)   │  → dev 환경
  └──────────┘     └──────────────┘
       │
       │           ┌──────────────┐
       └────────── │ ConfigMap    │
                   │ (prod 설정)  │  → prod 환경
                   └──────────────┘
```

### ConfigMap 생성

**명령형(Imperative) 방식**
```bash
# 리터럴 값으로 생성
kubectl create configmap my-config \
  --from-literal=DB_HOST=mysql-service \
  --from-literal=DB_PORT=3306 \
  --from-literal=LOG_LEVEL=info

# 파일로 생성
kubectl create configmap app-config --from-file=app.properties

# 특정 키 이름으로 파일 매핑
kubectl create configmap app-config --from-file=config.properties=./my-app.properties

# 디렉토리 내 모든 파일로 생성 (파일명이 키, 내용이 값)
kubectl create configmap app-config --from-file=./config-dir/

# env 파일로 생성 (KEY=VALUE 형식)
kubectl create configmap app-config --from-env-file=app.env
```

**선언형(Declarative) 방식**
```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-config
  namespace: default
data:                        # 일반 텍스트 데이터
  DB_HOST: mysql-service
  DB_PORT: "3306"            # 숫자도 문자열로 저장
  LOG_LEVEL: info
  app.properties: |          # 파일 내용도 저장 가능 (멀티라인)
    server.port=8080
    spring.datasource.url=jdbc:mysql://mysql:3306/mydb
    spring.profiles.active=production
binaryData:                  # 바이너리 데이터 (Base64 인코딩)
  logo.png: <base64-encoded>
```

```bash
# ConfigMap 확인
kubectl get configmap
kubectl describe configmap my-config
kubectl get configmap my-config -o yaml
```

### ConfigMap 사용 방식

```
ConfigMap 사용 방식 3가지:

1. 환경 변수로 주입
   ConfigMap ──(env)──→ 컨테이너 환경 변수

2. 볼륨 마운트로 파일 주입
   ConfigMap ──(volume)──→ 컨테이너 내 파일

3. 커맨드 인자로 주입
   ConfigMap ──(args)──→ 컨테이너 실행 인자
```

**1. 환경 변수 — 개별 키 주입 (valueFrom)**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: config-env-pod
spec:
  containers:
    - name: app
      image: my-app:v1
      env:
        - name: DATABASE_HOST          # 컨테이너에서 사용할 환경 변수명
          valueFrom:
            configMapKeyRef:
              name: my-config          # ConfigMap 이름
              key: DB_HOST             # ConfigMap 내 키
        - name: DATABASE_PORT
          valueFrom:
            configMapKeyRef:
              name: my-config
              key: DB_PORT
```

**2. 환경 변수 — 전체 키 일괄 주입 (envFrom)**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: config-envfrom-pod
spec:
  containers:
    - name: app
      image: my-app:v1
      envFrom:
        - configMapRef:
            name: my-config            # ConfigMap의 모든 키가 환경 변수로 주입
          prefix: APP_                  # 선택: 접두사 추가 → APP_DB_HOST, APP_DB_PORT
```

**3. 볼륨 마운트 — 파일로 주입**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: config-volume-pod
spec:
  containers:
    - name: app
      image: my-app:v1
      volumeMounts:
        - name: config-vol
          mountPath: /etc/config       # 이 디렉토리에 ConfigMap 키가 파일로 생성
          readOnly: true
  volumes:
    - name: config-vol
      configMap:
        name: my-config
        items:                         # 선택: 특정 키만 마운트
          - key: app.properties
            path: application.properties  # 파일명 변경 가능
```

```
볼륨 마운트 결과:
/etc/config/
 ├─ DB_HOST          → 내용: "mysql-service"
 ├─ DB_PORT          → 내용: "3306"
 ├─ LOG_LEVEL        → 내용: "info"
 └─ app.properties   → 내용: "server.port=8080\n..."

items를 지정하면:
/etc/config/
 └─ application.properties  → 내용: "server.port=8080\n..."
```

**4. 커맨드 인자로 주입**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: config-cmd-pod
spec:
  containers:
    - name: app
      image: my-app:v1
      command: ["./app"]
      args: ["--log-level=$(LOG_LEVEL)"]
      env:
        - name: LOG_LEVEL
          valueFrom:
            configMapKeyRef:
              name: my-config
              key: LOG_LEVEL
```

### ConfigMap 업데이트와 반영

| 주입 방식 | ConfigMap 수정 시 자동 반영 | 비고 |
| --- | --- | --- |
| 환경 변수 (env/envFrom) | **X** — Pod 재시작 필요 | 환경 변수는 프로세스 시작 시 결정 |
| 볼륨 마운트 | **O** — 자동 갱신 (수십 초~수 분) | kubelet의 sync 주기에 따름 |
| subPath 볼륨 마운트 | **X** — 자동 갱신 안됨 | subPath는 심볼릭 링크 아님 |

```bash
# ConfigMap 수정
kubectl edit configmap my-config
kubectl apply -f configmap.yaml

# 볼륨 마운트 시 갱신 확인 (Pod 내부)
watch cat /etc/config/LOG_LEVEL
```

### 주의사항

- ConfigMap은 **1MiB** 크기 제한
- ConfigMap은 **암호화되지 않음** → 민감 정보는 Secret 사용
- ConfigMap이 없는 상태에서 Pod가 참조하면 **Pod 생성 실패** (optional: true로 회피 가능)
- immutable ConfigMap (`immutable: true`)은 수정 불가 → 실수 방지, 성능 향상

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: immutable-config
immutable: true                # 한번 설정하면 수정 불가 (삭제 후 재생성만 가능)
data:
  ENVIRONMENT: production
```

---

## 애플리케이션에 시크릿(Secret)을 이용해 추가 설정을 암호화해서 넣기

**민감 정보(비밀번호, 토큰, 인증서 등)**를 안전하게 저장하고 Pod에 주입하는 메커니즘

```
ConfigMap vs Secret:

ConfigMap:
  평문 저장 → DB 호스트, 포트, 로그 레벨 등 일반 설정

Secret:
  Base64 인코딩 저장 → 비밀번호, API 키, TLS 인증서 등 민감 정보
  (etcd에서 암호화 가능, RBAC으로 접근 제어)

  ┌──────────┐     ┌──────────────┐
  │   Pod    │ ←── │ Secret       │
  │          │     │ DB_PASSWORD  │
  │          │     │ API_KEY      │
  └──────────┘     │ TLS_CERT     │
                   └──────────────┘
```

### Secret 타입

| 타입 | 설명 | 용도 |
| --- | --- | --- |
| `Opaque` | 기본 타입, 임의의 key-value | 비밀번호, API 키 등 |
| `kubernetes.io/dockerconfigjson` | Docker 레지스트리 인증 | 프라이빗 이미지 pull |
| `kubernetes.io/tls` | TLS 인증서 + 키 | Ingress TLS 등 |
| `kubernetes.io/basic-auth` | 기본 인증 | 사용자명/비밀번호 |
| `kubernetes.io/ssh-auth` | SSH 인증 | SSH 프라이빗 키 |
| `kubernetes.io/service-account-token` | SA 토큰 | API 서버 인증 (자동 생성) |

### Secret 생성

**명령형 방식**
```bash
# 리터럴 값으로 생성
kubectl create secret generic db-secret \
  --from-literal=username=admin \
  --from-literal=password='S3cur3P@ss!'

# 파일로 생성
kubectl create secret generic tls-secret \
  --from-file=tls.crt=./server.crt \
  --from-file=tls.key=./server.key

# TLS Secret 생성 (전용 명령)
kubectl create secret tls my-tls-secret \
  --cert=./server.crt \
  --key=./server.key

# Docker 레지스트리 Secret 생성
kubectl create secret docker-registry regcred \
  --docker-server=https://index.docker.io/v1/ \
  --docker-username=myuser \
  --docker-password=mypass \
  --docker-email=my@email.com
```

**선언형 방식**
```yaml
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
data:                          # Base64 인코딩된 값
  username: YWRtaW4=           # echo -n 'admin' | base64
  password: UzNjdXIzUEBzcyE=  # echo -n 'S3cur3P@ss!' | base64
---
# stringData를 사용하면 평문으로 작성 가능 (적용 시 자동 인코딩)
apiVersion: v1
kind: Secret
metadata:
  name: db-secret
type: Opaque
stringData:                    # 평문으로 작성 → 자동 Base64 인코딩
  username: admin
  password: S3cur3P@ss!
```

```bash
# Base64 인코딩/디코딩
echo -n 'admin' | base64              # YWRtaW4=
echo -n 'YWRtaW4=' | base64 -d        # admin

# Secret 확인 (값은 숨겨짐)
kubectl get secret db-secret
kubectl describe secret db-secret

# Secret 값 확인 (Base64 디코딩 필요)
kubectl get secret db-secret -o jsonpath='{.data.password}' | base64 -d
```

### Secret 사용 방식

ConfigMap과 동일한 3가지 방식 (환경 변수, 볼륨 마운트, 커맨드 인자)

**1. 환경 변수로 주입**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secret-env-pod
spec:
  containers:
    - name: app
      image: my-app:v1
      env:
        - name: DB_USERNAME
          valueFrom:
            secretKeyRef:              # configMapKeyRef 대신 secretKeyRef
              name: db-secret
              key: username
        - name: DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: db-secret
              key: password
```

**2. 볼륨 마운트로 파일 주입**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: secret-volume-pod
spec:
  containers:
    - name: app
      image: my-app:v1
      volumeMounts:
        - name: secret-vol
          mountPath: /etc/secrets
          readOnly: true
  volumes:
    - name: secret-vol
      secret:
        secretName: db-secret
        defaultMode: 0400              # 파일 권한 설정 (소유자 읽기만)
```

```
마운트 결과:
/etc/secrets/
 ├─ username  → 내용: "admin" (자동 디코딩)
 └─ password  → 내용: "S3cur3P@ss!" (자동 디코딩)
```

**3. 프라이빗 레지스트리 이미지 Pull**
```yaml
apiVersion: v1
kind: Pod
metadata:
  name: private-image-pod
spec:
  imagePullSecrets:
    - name: regcred                    # docker-registry Secret 참조
  containers:
    - name: app
      image: my-private-registry.com/my-app:v1
```

### Secret 보안 강화

```
Secret의 보안 한계:
  Base64는 인코딩일 뿐 암호화가 아님!
  etcd에 평문(또는 Base64) 저장 → 누구든 etcd 접근하면 비밀 노출

보안 강화 방법:
  1. etcd 암호화 (EncryptionConfiguration)
  2. RBAC으로 Secret 접근 제어
  3. 외부 시크릿 관리자 연동 (Vault, AWS Secrets Manager 등)
```

**etcd 암호화 설정**
```yaml
# /etc/kubernetes/encryption-config.yaml
apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: <BASE64_ENCODED_32BYTE_KEY>
      - identity: {}           # 암호화되지 않은 기존 Secret 읽기용
```

| 보안 조치 | 설명 |
| --- | --- |
| RBAC | Secret 리소스에 대한 get/list 권한 최소화 |
| etcd 암호화 | `EncryptionConfiguration`으로 저장 시 암호화 |
| 감사 로그 | Secret 접근 이벤트 로깅 |
| 네임스페이스 격리 | Secret은 네임스페이스 범위 — 다른 NS에서 접근 불가 |
| 외부 연동 | HashiCorp Vault, AWS Secrets Manager, Sealed Secrets 등 |

---

## 애플리케이션 롤링 업데이트(set, rollout)

**무중단으로 애플리케이션을 새 버전으로 교체**하는 배포 전략

```
롤링 업데이트 과정 (maxSurge=1, maxUnavailable=0):

Step 1:  [v1] [v1] [v1]              ← 현재 상태 (replicas=3)
Step 2:  [v1] [v1] [v1] [v2]         ← 새 Pod 1개 추가 (maxSurge=1)
Step 3:  [v1] [v1] [  ] [v2]         ← v1 Pod 1개 종료
Step 4:  [v1] [v1] [v2] [v2]         ← 새 Pod 1개 추가
Step 5:  [v1] [  ] [v2] [v2]         ← v1 Pod 1개 종료
Step 6:  [v1] [v2] [v2] [v2]         ← 새 Pod 1개 추가
Step 7:  [  ] [v2] [v2] [v2]         ← v1 Pod 1개 종료
Step 8:  [v2] [v2] [v2]              ← 완료!
```

### Deployment의 배포 전략

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 3
  strategy:
    type: RollingUpdate             # 기본값
    rollingUpdate:
      maxSurge: 1                   # 최대 초과 Pod 수 (또는 비율 "25%")
      maxUnavailable: 0             # 최대 불가용 Pod 수 (또는 비율 "25%")
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: app
          image: my-app:v1
```

**strategy 타입**
| 타입 | 설명 | 다운타임 |
| --- | --- | --- |
| `RollingUpdate` (기본) | 점진적으로 새 Pod 생성 + 구 Pod 제거 | 없음 |
| `Recreate` | 기존 Pod 모두 삭제 후 새 Pod 생성 | 있음 |

**maxSurge와 maxUnavailable**
| 파라미터 | 의미 | 기본값 |
| --- | --- | --- |
| `maxSurge` | replicas 수를 초과하여 생성 가능한 Pod 수 | 25% |
| `maxUnavailable` | 업데이트 중 사용 불가능할 수 있는 Pod 수 | 25% |

```
예시: replicas=4

maxSurge=25%, maxUnavailable=25%:
  최대 Pod 수: 4 + 1(25%) = 5
  최소 가용 Pod: 4 - 1(25%) = 3
  → 동시에 최대 5개 Pod, 최소 3개 가용

maxSurge=1, maxUnavailable=0:
  최대 Pod 수: 4 + 1 = 5
  최소 가용 Pod: 4 - 0 = 4
  → 항상 4개 가용 보장 (가장 안전하지만 느림)

maxSurge=0, maxUnavailable=1:
  최대 Pod 수: 4 + 0 = 4
  최소 가용 Pod: 4 - 1 = 3
  → 리소스 추가 없이 배포 (리소스 제한 환경)
```

### kubectl set으로 이미지 업데이트

```bash
# 이미지 변경 → 롤링 업데이트 시작
kubectl set image deployment/my-app app=my-app:v2

# 여러 컨테이너 동시 변경
kubectl set image deployment/my-app app=my-app:v2 sidecar=sidecar:v3

# 업데이트 사유 기록 (--record는 deprecated → annotation 사용)
kubectl annotate deployment/my-app kubernetes.io/change-cause="Update to v2"
```

### kubectl rollout으로 배포 관리

```bash
# 배포 상태 확인
kubectl rollout status deployment/my-app
# Waiting for deployment "my-app" rollout to finish: 1 out of 3 new replicas have been updated...
# deployment "my-app" successfully rolled out

# 배포 일시 중지 (카나리 배포처럼 활용)
kubectl rollout pause deployment/my-app

# 일시 중지 상태에서 여러 변경 적용
kubectl set image deployment/my-app app=my-app:v2
kubectl set resources deployment/my-app -c app --limits=cpu=500m,memory=256Mi

# 배포 재개 (일괄 적용)
kubectl rollout resume deployment/my-app

# 배포 이력 확인
kubectl rollout history deployment/my-app
# REVISION  CHANGE-CAUSE
# 1         <none>
# 2         Update to v2
# 3         Update to v3

# 특정 리비전 상세 확인
kubectl rollout history deployment/my-app --revision=2
```

### 롤백 (rollout undo)

```
롤백 흐름:
  v3 (현재, 문제 발생!) → rollout undo → v2 (이전 버전으로 복귀)

  [v3] [v3] [v3]  →  rollout undo  →  [v2] [v2] [v2]
```

```bash
# 직전 버전으로 롤백
kubectl rollout undo deployment/my-app

# 특정 리비전으로 롤백
kubectl rollout undo deployment/my-app --to-revision=1

# 롤백 후 상태 확인
kubectl rollout status deployment/my-app
```

### revisionHistoryLimit

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  revisionHistoryLimit: 10          # 보관할 ReplicaSet 수 (기본값: 10)
  # ...
```

- Deployment는 이전 버전의 **ReplicaSet을 보관**하여 롤백 가능
- `revisionHistoryLimit`이 0이면 롤백 불가
- 오래된 ReplicaSet은 자동 삭제

### 배포 과정에서 내부 동작

```
kubectl set image → Deployment 업데이트
                        │
                        ▼
              새 ReplicaSet 생성 (replicas: 0 → 점진 증가)
                        │
                        ▼
              기존 ReplicaSet 축소 (replicas: 3 → 점진 감소)

Deployment
 ├─ ReplicaSet (v1, replicas: 0)  ← 보관 (롤백용)
 ├─ ReplicaSet (v2, replicas: 0)  ← 보관
 └─ ReplicaSet (v3, replicas: 3)  ← 현재 활성
```

```bash
# ReplicaSet 확인
kubectl get replicaset
# NAME                   DESIRED   CURRENT   READY   AGE
# my-app-6b7d8f9c5d     3         3         3       1m   ← 현재 (v3)
# my-app-5c8d7e4b3a     0         0         0       1h   ← 이전 (v2)
# my-app-4a7c6d3e2f     0         0         0       2h   ← 이전 (v1)
```

### minReadySeconds

```yaml
spec:
  minReadySeconds: 30              # 새 Pod가 Ready 후 30초 대기 후 다음 진행
  progressDeadlineSeconds: 600     # 10분 내 배포 미완료 시 실패 처리
```

- `minReadySeconds`: 새 Pod가 Ready 상태가 된 후 **추가 대기 시간** (안정성 확인)
- `progressDeadlineSeconds`: 배포가 진행되지 않으면 실패로 판단하는 시간 (기본 600초)

---

## 애플리케이션을 동적으로 배포하기(kustomize)

**기본 YAML(base)을 유지하면서 환경별로 다른 설정을 오버레이(overlay)**하는 도구

```
기존 문제:
  dev.yaml, staging.yaml, prod.yaml → 중복이 많고 관리 힘듦

Kustomize 해결:
  base (공통) + overlay (환경별 차이만) → 조합하여 최종 YAML 생성

  base/
   ├─ deployment.yaml     ← 공통 설정
   ├─ service.yaml
   └─ kustomization.yaml

  overlays/
   ├─ dev/
   │   └─ kustomization.yaml   ← replicas: 1, 이미지 태그: dev
   ├─ staging/
   │   └─ kustomization.yaml   ← replicas: 2, 이미지 태그: staging
   └─ prod/
       └─ kustomization.yaml   ← replicas: 5, 이미지 태그: v1.2.3
```

### 디렉토리 구조

```
my-app/
 ├─ base/
 │   ├─ deployment.yaml
 │   ├─ service.yaml
 │   ├─ configmap.yaml
 │   └─ kustomization.yaml        ← base 리소스 목록
 └─ overlays/
     ├─ dev/
     │   ├─ kustomization.yaml     ← base 참조 + dev 패치
     │   └─ replica-patch.yaml
     └─ prod/
         ├─ kustomization.yaml     ← base 참조 + prod 패치
         └─ replica-patch.yaml
```

### base 설정

```yaml
# base/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-app
spec:
  replicas: 1
  selector:
    matchLabels:
      app: my-app
  template:
    metadata:
      labels:
        app: my-app
    spec:
      containers:
        - name: app
          image: my-app:latest
          ports:
            - containerPort: 8080
```

```yaml
# base/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-app
spec:
  selector:
    app: my-app
  ports:
    - port: 80
      targetPort: 8080
```

```yaml
# base/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - deployment.yaml
  - service.yaml
```

### overlay 설정

```yaml
# overlays/dev/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../../base                       # base 디렉토리 참조
namePrefix: dev-                     # 모든 리소스 이름에 접두사
namespace: dev                       # 네임스페이스 지정
commonLabels:                        # 공통 레이블 추가
  environment: dev
images:                              # 이미지 태그 변경
  - name: my-app
    newTag: dev-latest
patches:                             # 패치 적용
  - target:
      kind: Deployment
      name: my-app
    patch: |
      - op: replace
        path: /spec/replicas
        value: 1
```

```yaml
# overlays/prod/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../../base
namePrefix: prod-
namespace: production
commonLabels:
  environment: production
images:
  - name: my-app
    newTag: v1.2.3
patches:
  - target:
      kind: Deployment
      name: my-app
    patch: |
      - op: replace
        path: /spec/replicas
        value: 5
```

### Kustomize 주요 기능

| 기능 | 설명 | 예시 |
| --- | --- | --- |
| `namePrefix` / `nameSuffix` | 리소스 이름에 접두사/접미사 | `dev-my-app` |
| `namespace` | 네임스페이스 일괄 지정 | `namespace: dev` |
| `commonLabels` | 모든 리소스에 레이블 추가 | `env: dev` |
| `commonAnnotations` | 모든 리소스에 어노테이션 추가 | |
| `images` | 이미지 이름/태그 변경 | `newTag: v2.0.0` |
| `configMapGenerator` | ConfigMap 자동 생성 | 파일/리터럴에서 생성 |
| `secretGenerator` | Secret 자동 생성 | 파일/리터럴에서 생성 |
| `patches` | 리소스 부분 수정 | JSON Patch, Strategic Merge Patch |

**configMapGenerator / secretGenerator**
```yaml
# kustomization.yaml
configMapGenerator:
  - name: app-config
    literals:
      - DB_HOST=mysql
      - DB_PORT=3306
    files:
      - app.properties
    # → 이름에 해시 suffix 자동 추가: app-config-8g2c5k7h
    #   ConfigMap 변경 시 새 이름 → Pod 자동 재시작

secretGenerator:
  - name: db-secret
    literals:
      - username=admin
      - password=secret123
    type: Opaque
```

### kubectl 사용

```bash
# 빌드 결과 미리보기 (실제 적용 안함)
kubectl kustomize overlays/dev

# 적용
kubectl apply -k overlays/dev

# 삭제
kubectl delete -k overlays/dev

# kustomize 독립 실행 (kubectl에 내장된 것보다 최신 버전 사용 가능)
kustomize build overlays/prod | kubectl apply -f -
```

```
kubectl kustomize overlays/dev 실행 결과:

base의 deployment.yaml + overlay의 패치가 합쳐진 최종 YAML 출력:
  - name: dev-my-app (접두사 추가)
  - namespace: dev
  - replicas: 1
  - image: my-app:dev-latest
  - labels에 environment: dev 추가
```

---

## 애플리케이션을 더욱더 쉽게 배포하기(helm)

Kubernetes의 **패키지 매니저** — 복잡한 애플리케이션을 하나의 패키지(Chart)로 관리

```
비유:
  apt/yum이 리눅스의 패키지 매니저인 것처럼
  Helm은 쿠버네티스의 패키지 매니저

  apt install nginx      →  helm install my-nginx bitnami/nginx
  apt upgrade nginx      →  helm upgrade my-nginx bitnami/nginx
  apt remove nginx       →  helm uninstall my-nginx

Helm의 3가지 핵심 개념:
  Chart      → 패키지 (YAML 템플릿 + 기본값)
  Release    → Chart의 설치 인스턴스 (같은 Chart를 여러 번 설치 가능)
  Repository → Chart 저장소 (bitnami, stable 등)
```

### Chart 구조

```
my-chart/
 ├─ Chart.yaml              # 차트 메타데이터 (이름, 버전, 설명)
 ├─ values.yaml             # 기본 설정값
 ├─ templates/              # Kubernetes 매니페스트 템플릿
 │   ├─ deployment.yaml
 │   ├─ service.yaml
 │   ├─ ingress.yaml
 │   ├─ configmap.yaml
 │   ├─ _helpers.tpl        # 템플릿 헬퍼 함수
 │   └─ NOTES.txt           # 설치 후 안내 메시지
 ├─ charts/                 # 의존 차트 (서브차트)
 └─ .helmignore             # 패키징 시 제외 파일
```

**Chart.yaml**
```yaml
apiVersion: v2
name: my-app
description: My Application Helm Chart
type: application
version: 0.1.0          # 차트 버전
appVersion: "1.2.3"     # 애플리케이션 버전
dependencies:
  - name: mysql
    version: "9.x.x"
    repository: https://charts.bitnami.com/bitnami
```

**values.yaml (기본값)**
```yaml
replicaCount: 3
image:
  repository: my-app
  tag: "v1.2.3"
  pullPolicy: IfNotPresent
service:
  type: ClusterIP
  port: 80
resources:
  limits:
    cpu: 500m
    memory: 256Mi
  requests:
    cpu: 250m
    memory: 128Mi
```

**templates/deployment.yaml (Go 템플릿)**
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "my-chart.fullname" . }}
  labels:
    {{- include "my-chart.labels" . | nindent 4 }}
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels:
      {{- include "my-chart.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "my-chart.selectorLabels" . | nindent 8 }}
    spec:
      containers:
        - name: {{ .Chart.Name }}
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports:
            - containerPort: 8080
          resources:
            {{- toYaml .Values.resources | nindent 12 }}
```

### Helm 기본 명령어

```bash
# 리포지토리 추가
helm repo add bitnami https://charts.bitnami.com/bitnami
helm repo update

# 차트 검색
helm search repo nginx
helm search hub wordpress           # Artifact Hub 검색

# 차트 설치
helm install my-release bitnami/nginx
helm install my-release bitnami/nginx -n my-namespace --create-namespace

# values 오버라이드 (--set 또는 -f)
helm install my-release bitnami/nginx \
  --set replicaCount=3 \
  --set service.type=LoadBalancer

helm install my-release bitnami/nginx -f my-values.yaml

# 설치 전 렌더링 확인 (--dry-run)
helm install my-release bitnami/nginx --dry-run

# 템플릿 렌더링만 확인 (클러스터 접근 불필요)
helm template my-release bitnami/nginx -f my-values.yaml
```

### Release 관리

```bash
# 릴리스 목록
helm list
helm list -n my-namespace
helm list --all-namespaces

# 릴리스 상태 확인
helm status my-release

# 업그레이드
helm upgrade my-release bitnami/nginx --set replicaCount=5
helm upgrade my-release bitnami/nginx -f new-values.yaml

# 업그레이드 + 없으면 설치 (CI/CD에서 유용)
helm upgrade --install my-release bitnami/nginx -f values.yaml

# 롤백
helm rollback my-release                # 직전 리비전으로
helm rollback my-release 1              # 특정 리비전으로

# 리비전 이력 확인
helm history my-release

# 삭제
helm uninstall my-release
```

### Kustomize vs Helm 비교

| 비교 항목 | Kustomize | Helm |
| --- | --- | --- |
| 접근 방식 | 패치/오버레이 (YAML 직접 수정) | 템플릿 엔진 (Go template) |
| 학습 곡선 | 낮음 (순수 YAML) | 중간 (Go 템플릿 문법) |
| 패키지 관리 | 없음 | Chart 리포지토리 |
| 릴리스 관리 | 없음 | 릴리스 버전 관리, 롤백 |
| 의존성 관리 | 없음 | 서브차트, dependencies |
| 내장 여부 | kubectl에 내장 (`-k`) | 별도 설치 필요 |
| 적합한 상황 | 환경별 설정 차이가 적을 때 | 복잡한 앱, 재사용/배포 패키지 |

---

## 애플리케이션의 자원 사용량 확인하기(메트릭 서버 / Metrics-server)

**클러스터 내 노드와 Pod의 CPU/메모리 사용량을 수집**하는 경량 모니터링 컴포넌트

```
Metrics Server 역할:

각 노드의 kubelet (cAdvisor)
  ├─ Node-1 kubelet ──→ CPU/Memory 메트릭
  ├─ Node-2 kubelet ──→ CPU/Memory 메트릭
  └─ Node-3 kubelet ──→ CPU/Memory 메트릭
          │
          ▼
    Metrics Server (수집 & 집계)
          │
          ▼
    Metrics API (/apis/metrics.k8s.io/v1beta1)
          │
          ├──→ kubectl top         (사용자 조회)
          ├──→ HPA                 (자동 스케일링 판단)
          └──→ VPA                 (자동 리소스 조정)
```

### 설치

```bash
# Metrics Server 설치
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml

# 또는 Helm으로 설치
helm repo add metrics-server https://kubernetes-sigs.github.io/metrics-server/
helm install metrics-server metrics-server/metrics-server -n kube-system

# 설치 확인
kubectl get deployment metrics-server -n kube-system
kubectl get apiservice v1beta1.metrics.k8s.io
# AVAILABLE가 True여야 함
```

**minikube / 테스트 환경에서 TLS 오류 시**
```bash
# --kubelet-insecure-tls 옵션 추가 필요
kubectl edit deployment metrics-server -n kube-system
# args에 추가:
#   - --kubelet-insecure-tls
```

### kubectl top 명령어

```bash
# 노드 자원 사용량
kubectl top nodes
# NAME     CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
# node-1   250m         12%    1024Mi          52%
# node-2   180m         9%     896Mi           46%
# node-3   320m         16%    1280Mi          65%

# Pod 자원 사용량
kubectl top pods
kubectl top pods -n kube-system
kubectl top pods --all-namespaces
kubectl top pods -l app=web                     # 레이블 필터

# 컨테이너별 사용량
kubectl top pods --containers

# 정렬
kubectl top pods --sort-by=cpu
kubectl top pods --sort-by=memory
```

### Metrics Server 특징

| 항목 | 설명 |
| --- | --- |
| 수집 주기 | 기본 15초 간격 |
| 저장 기간 | **메모리에만 저장** — 최신 값만 유지 (히스토리 없음) |
| 용도 | kubectl top, HPA, VPA의 실시간 메트릭 소스 |
| 한계 | 과거 데이터 조회 불가 → 장기 모니터링은 Prometheus 등 사용 |

```
모니터링 도구 역할 분담:

Metrics Server → 실시간 리소스 사용량 (HPA, kubectl top)
Prometheus     → 시계열 메트릭 수집, 장기 저장, 알림
Grafana        → 대시보드 시각화
```

---

## 자원 상태에 따라 애플리케이션을 자동으로 조절하기(HPA)

**Horizontal Pod Autoscaler** — Pod의 리소스 사용량이나 커스텀 메트릭에 따라 **Pod 수를 자동 조절**

```
HPA 동작 원리:

  Metrics Server ──(메트릭 수집)──→ HPA Controller
                                        │
                                   현재 CPU: 80%
                                   목표 CPU: 50%
                                        │
                                        ▼
                                  "Pod가 부족하다"
                                        │
                                        ▼
                              Deployment replicas 증가
                                   3 → 5개

  부하 증가 시 (Scale Out):
    [Pod] [Pod] [Pod]  →  [Pod] [Pod] [Pod] [Pod] [Pod]

  부하 감소 시 (Scale In):
    [Pod] [Pod] [Pod] [Pod] [Pod]  →  [Pod] [Pod] [Pod]
```

### HPA 생성

**명령형 방식**
```bash
# CPU 사용률 50% 기준, 최소 2개 ~ 최대 10개
kubectl autoscale deployment my-app \
  --cpu-percent=50 \
  --min=2 \
  --max=10
```

**선언형 방식 (v2)**
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50     # 평균 CPU 사용률 50% 유지 목표
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 70     # 평균 메모리 사용률 70% 유지 목표
```

### 스케일링 계산 공식

```
필요 Pod 수 = ceil(현재 Pod 수 × (현재 메트릭 값 / 목표 메트릭 값))

예시:
  현재 Pod 수: 3
  현재 평균 CPU: 90%
  목표 CPU: 50%

  필요 Pod 수 = ceil(3 × (90 / 50)) = ceil(5.4) = 6
  → 3개 → 6개로 스케일 아웃
```

### HPA 동작 조건

- **전제: Pod에 resources.requests가 반드시 설정**되어 있어야 함
  - requests 대비 사용률(%)을 계산하기 때문
- **Metrics Server가 설치**되어 있어야 함

```yaml
# HPA가 동작하려면 Pod에 requests 설정 필수
spec:
  containers:
    - name: app
      image: my-app:v1
      resources:
        requests:
          cpu: "200m"          # 이 값 대비 사용률 계산
          memory: "256Mi"
        limits:
          cpu: "500m"
          memory: "512Mi"
```

### 메트릭 타입

```yaml
metrics:
  # 1. Resource — CPU/Memory (가장 일반적)
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization              # requests 대비 퍼센트
        averageUtilization: 50

  # 2. Resource — 절대값 기준
  - type: Resource
    resource:
      name: memory
      target:
        type: AverageValue             # Pod당 평균 사용량
        averageValue: 500Mi

  # 3. Pods — 커스텀 메트릭 (Pod 단위)
  - type: Pods
    pods:
      metric:
        name: packets-per-second
      target:
        type: AverageValue
        averageValue: 1k

  # 4. Object — 외부 오브젝트 메트릭
  - type: Object
    object:
      describedObject:
        apiVersion: networking.k8s.io/v1
        kind: Ingress
        name: main-route
      metric:
        name: requests-per-second
      target:
        type: Value
        value: 10k
```

### 스케일링 동작 제어 (behavior)

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: my-app-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: my-app
  minReplicas: 2
  maxReplicas: 20
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0      # 즉시 스케일 아웃
      policies:
        - type: Percent
          value: 100                     # 현재의 100%까지 한 번에 증가
          periodSeconds: 60
        - type: Pods
          value: 4                       # 또는 한 번에 최대 4개 추가
          periodSeconds: 60
      selectPolicy: Max                  # 두 정책 중 더 많은 쪽 선택
    scaleDown:
      stabilizationWindowSeconds: 300    # 5분간 안정화 후 스케일 다운
      policies:
        - type: Percent
          value: 10                      # 한 번에 10%씩만 감소
          periodSeconds: 60
```

```
behavior 동작:

scaleUp (확장):
  stabilizationWindowSeconds: 0 → 즉시 반응
  → 부하 증가에 빠르게 대응

scaleDown (축소):
  stabilizationWindowSeconds: 300 → 5분 대기
  → 일시적 부하 감소에 과도하게 반응하지 않음
  → "플래핑(flapping)" 방지
```

### HPA 확인 및 디버깅

```bash
# HPA 상태 확인
kubectl get hpa
# NAME         REFERENCE       TARGETS   MINPODS   MAXPODS   REPLICAS   AGE
# my-app-hpa   Deployment/my-app   45%/50%   2         10        3       5m

# HPA 상세 확인 (이벤트, 조건 등)
kubectl describe hpa my-app-hpa

# TARGETS가 <unknown>/50% 이면:
#   - Metrics Server 미설치
#   - Pod에 resources.requests 미설정
#   - 메트릭 수집 전 (잠시 대기)
```

### HPA 주의사항

| 주의사항 | 설명 |
| --- | --- |
| requests 필수 | Pod에 resources.requests가 없으면 CPU 사용률 계산 불가 |
| Metrics Server 필수 | HPA의 메트릭 소스 |
| 쿨다운 기간 | 기본 스케일 다운 안정화: 5분 (급격한 축소 방지) |
| HPA + 수동 스케일링 충돌 | HPA 활성 중 `kubectl scale`은 HPA가 덮어씀 |
| Deployment만 대상 | ReplicaSet, StatefulSet도 가능하지만 주로 Deployment에 사용 |
| VPA와 동시 사용 주의 | HPA(수평)와 VPA(수직)가 같은 메트릭(CPU)을 사용하면 충돌 |

---

## 애플리케이션을 웹UI에서 관리하기(kube dashboard)

Kubernetes 클러스터를 **웹 브라우저에서 시각적으로 관리**할 수 있는 공식 대시보드

```
Kubernetes Dashboard:

  브라우저 ──(HTTPS)──→ Dashboard Pod ──(API)──→ API Server
                              │
                              ▼
                    ┌─────────────────────┐
                    │  Dashboard UI       │
                    │  ├─ 워크로드 조회    │
                    │  ├─ Pod 로그 확인    │
                    │  ├─ 리소스 생성/수정  │
                    │  ├─ 네임스페이스 전환  │
                    │  └─ 리소스 삭제      │
                    └─────────────────────┘
```

### 설치

```bash
# 공식 Dashboard 설치 (최신 버전 확인 후 적용)
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml

# Helm으로 설치
helm repo add kubernetes-dashboard https://kubernetes.github.io/dashboard/
helm install kubernetes-dashboard kubernetes-dashboard/kubernetes-dashboard \
  --namespace kubernetes-dashboard --create-namespace

# 설치 확인
kubectl get pods -n kubernetes-dashboard
kubectl get services -n kubernetes-dashboard
```

### 접속 방법

**1. kubectl proxy (로컬 개발용)**
```bash
kubectl proxy
# → http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
```

**2. NodePort로 외부 노출**
```bash
kubectl edit service kubernetes-dashboard -n kubernetes-dashboard
# type: ClusterIP → type: NodePort
# → https://<노드IP>:<NodePort>
```

**3. Port Forward**
```bash
kubectl port-forward -n kubernetes-dashboard service/kubernetes-dashboard 8443:443
# → https://localhost:8443
```

### 인증 설정 (ServiceAccount + Token)

Dashboard 접속 시 **토큰 기반 인증** 필요

```yaml
# 1. ServiceAccount 생성
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dashboard-admin
  namespace: kubernetes-dashboard
---
# 2. ClusterRoleBinding (cluster-admin 권한 부여)
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: dashboard-admin-binding
subjects:
  - kind: ServiceAccount
    name: dashboard-admin
    namespace: kubernetes-dashboard
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
```

```bash
# 토큰 생성 (v1.24+)
kubectl create token dashboard-admin -n kubernetes-dashboard

# 또는 장기 토큰 생성
kubectl apply -f - <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: dashboard-admin-token
  namespace: kubernetes-dashboard
  annotations:
    kubernetes.io/service-account.name: dashboard-admin
type: kubernetes.io/service-account-token
EOF

# 토큰 확인
kubectl get secret dashboard-admin-token -n kubernetes-dashboard -o jsonpath='{.data.token}' | base64 -d
```

### Dashboard 주요 기능

| 기능 | 설명 |
| --- | --- |
| 워크로드 조회 | Deployment, Pod, ReplicaSet, Job 등 상태 확인 |
| 리소스 생성 | YAML 에디터 또는 폼으로 리소스 생성 |
| Pod 로그 | 실시간 로그 스트리밍 |
| Pod exec | 웹 터미널에서 컨테이너 접속 |
| 네임스페이스 전환 | 드롭다운으로 네임스페이스 선택 |
| 리소스 수정/삭제 | YAML 편집 및 삭제 |
| 클러스터 상태 | 노드, PV, StorageClass 등 확인 |

### 보안 주의사항

```
Dashboard 보안 체크리스트:

✅ 외부 인터넷에 직접 노출하지 않기
  → kubectl proxy, port-forward, VPN 경유로 접속

✅ 최소 권한 원칙
  → cluster-admin 대신 필요한 권한만 부여
  → namespace별 Role + RoleBinding 사용

✅ HTTPS 사용
  → Dashboard는 기본적으로 HTTPS

✅ 네트워크 정책 적용
  → Dashboard Pod에 대한 접근을 제한

❌ skip 로그인 비활성화
  → --enable-skip-login 사용 금지 (프로덕션)
```

**읽기 전용 Dashboard 사용자 설정**
```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: dashboard-viewer
rules:
  - apiGroups: [""]
    resources: ["pods", "services", "configmaps", "namespaces", "nodes"]
    verbs: ["get", "list", "watch"]
  - apiGroups: ["apps"]
    resources: ["deployments", "replicasets", "statefulsets"]
    verbs: ["get", "list", "watch"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: dashboard-viewer-binding
subjects:
  - kind: ServiceAccount
    name: dashboard-viewer
    namespace: kubernetes-dashboard
roleRef:
  kind: ClusterRole
  name: dashboard-viewer
  apiGroup: rbac.authorization.k8s.io
```
