# 4장 처리율 제한 장치의 설계

네트워크 시스템의 처리율 제한 장치는 외부 요청에 대해 처리율을 제어하기 위한 장치이다. 이러한 제어 장치를 통해서 서버의 부하 또는 외부 api에 대한 비용을 최적화할 수 있다.

처리율 제어 장치는 클라이언트, 서버 또는 미들웨어에 위치할 수 있다. 다만, 클라이언트에서 설정하는 경우 변조가 쉽기 때문에 잘 사용되지 않는 방법이다.

## 처리율 제한 알고리즘

처리율 제한 알고리즘의 종류는 총 5가지가 존재한다.

* 토큰 버킷(token bucket)
* 누출 버킷(leaky bucket)
* 고정 윈도 카운터(fixed window counter)
* 이동 윈도 로그(sliding window log)
* 이동 윈도 카운터(sliding window counter)

### 토큰 버킷(token bucket)

* 버킷 안에 일정 갯수만큼 토큰을 넣어두고, 일정 시간마다 토큰이 채워지는 알고리즘
* 각 요청은 하나의 토큰을 사용하며 버킷에 토큰이 없으면 요청이 버려지게 됨
* 피크 트레픽은 허용하면서, 전체적인 평균 트래픽은 유지하는 데 적합함

### 누출 버킷(leaky bucket)

* 요청을 담아두는 큐를 두고, 해당 큐에서 일정시간마다 요청을 꺼내 처리하는 알고리즘
* 아무리 피크 요청이 들어오더라도 일정 속도로 응답을 주기 때문에, 네트워크/서버 부하를 일정하게 유지할 수 있음
* 다만, 큐가 가득차게되면 이후 요청들은 버려지게 됨

### 고정 윈도 카운터(fixed window counter)

* 특정 시간 단위를 지정한 다음, 해당 단위 내에 처리할 수 있는 요청의 수를 지정하는 알고리즘
* 특정 윈도우에 설정된 요청수만큼 처리할 수 있지만, 그보다 더 커지게 되면 요청이 버려지게 됨
* 윈도우간 경계에서 피크 요청이 들어오게 되면, 그 시점에 서버가 처리할 수 있는 요청보다 더 많이 들어오게될 수 있음. 이로인해, 서버의 리소스 사용량을 예측하기 어려움
    * 예를들면, 1분당 10개의 요청만 받을 수 있을때, 00초 부근에 각각 5초전에 7개, 5초 후에 7개를 받게 되면, 총 14개의 요청을 짧은 시간 내에 처리하게 됨

### 이동 윈도 로그(sliding window log)

* 각 요청 시간에 대해 타임스탬프를 로그로서 저장해두고, 현재 시간으로 부터 N개의 시간만 저장하는 알고리즘
* 요청이 들어올 때, 로그에서 오래된 타임스탬프를 제거하고, 남아있는 데이터가 임계값보다 크면, 해당 요청은 버려지게 됨
    * 오래된 타임스템프를 제거하는 기준은 윈도우의 크기에 따라 결정 됨. 즉, "현재 시간 - 윈도우 크기"의 이전에 저장된 시간을 제거하게 됨
* "특정 시간동안 N개의 요청 이하로 처리"라는 요구사항은 만족하지만, 피크 요청이 들어오면 모든 요청에 대해 로그로 저장하게 되어 메모리 사용량이 매우 큼

### 이동 윈도 카운터(sliding window counter)

* 시간을 고정된 크기로 나누고, 각 구간에 대해서 요청수를 저장. 또한, 최근 N초의 요청 수를 "현재 윈도우"와 "이전 윈도우"의 카운트를 가중 평균으로 합산해 근사값으로 계산하는 알고리즘
* 계산식으로는 "현재 윈도우 요청 수 + (직전 윈도우 요청 수 * 현재 시간이 겹치는 비율)
* 메모리 사용량이 "이동 윈도우 로그"에 비해 상대적으로 적음
* 윈도우 경계간 버스트 문제도 해결할 수 있음


## 응답 헤더

처리율 제한으로 인해 429(too many request)를 반환할 때, 클라이언트가 언제 다시 시도할 수 있을지 알 수 있도록 재시도 정보가 담긴 헤더를 내려줘야 한다. 또는 남은 요청 가능 수를 반환해 추가 요청이 들어오지 않도록 남은 시도 수가 담긴 헤더를 내려줘야 한다.

* X-Ratelimit-Remaining: 윈도 내에 남은 처리 가능 요청의 수
* X-Ratelimit-Limit: 매 윈도마다 클라이언트가 전송할 수 있는 요청의 수
* X-Ratelimit-Retry-After: 한도 제한에 걸리지 않으려면 몇 초 뒤에 요청을 다시 보내야 하는지 알림.


## 분산환경에서의 처리율 제어

각 요청들에 대해서 처리율을 계산할 때, 만일 제어기가 여러대로 구성되어 있으면, 중앙화된 저장소가 필요하다. 일반적으로 redis와 같은 메모리 데이터베이스를 통해 처리한다. 다만, 데이터 동기화로 최종적 일관성을 사용할수도 있다.


# ETC

## 왜 강한 일관성이 아닌 최종적 일관성을 선택하는가?

강한 일관성을 선택하면, 장애가 발생할 때 이에 대한 영향 범위가 커질 수 있음. 대신 최종적 일관성을 사용하는 이유는 요청수에 대해서 오차가 발생할 수 있지만, 각 처리기에 장애가 전파되지 않도록 하기 위함이 큼

최종적 일관성을 구현하는 방법으로는 다음과 같은 방법이 있다.

* 중앙 Redis + 원자 연산
    * Lua 스크립트를 통해 원자적으로 수행해 race condition을 줄임
    * Redis의 클러스터간 복제는 최종적 일관성을 가지기 때문에 약간의 지연이 발생함
    * 즉, 특정 리전에 장애가 발생하는 경우 다른 리전으로 자동으로 라우팅되도록 구성하되, 라우팅 될 리전에는 최종적 일관성에 의해 장애 시점의 데이터가 있을지 확정할 수 없다. 일부 오차가 발생할 수 있음
* 로컬 캐시 + 주기적 동기화
    * 중앙 저장소가 있고, 이를 동기화해 로컬 캐시에 저장하는 방법
    * 네트워크를 타지 않아 지연은 적지만, 중앙 저장소에 장애가 발생하면 동기화가 중단되어 각 처리기가 로컬 상태만으로 독립적으로 계산하게 된다. 이로 인해 전체적으로는 설정값보다 더 많은 요청을 허용하게 될 수 있으며, 반대로 이를 의도적으로 활용해 장애 시에는 fallback 전략으로 각 처리기가 처리량을 분담해 개별적으로 요청을 나눠 처리하도록 설계할 수도 있음

## Redis 클러스터에 lua 스크립트 요청을 보내면 어떻게 처리되나요?

lua 스크립트는 스크립트 내부에서 사용하는 키들 모두가 동일한 노드에 있어야지 실행 가능함. 따라서, 캐시 키를 설계할 때, `{}`를 잘 활용해 특정 노드에 데이터가 모두 저장되도록 해야한다.

## Redis Master, Slave 구조에서 Slave -> Master로 승격될 때, replication lag로 인해 데이터가 유실되면 어떻게 할건가요?

유실을 완전히 막을 수 없음. 다만 replication lag가 일정 값보다 큰. 즉 뒤쳐진 slave는 master로 승격될 수 없도록 설정해야 한다.

또한, slave -> master로 승격될 때, redis는 알림 또는 이벤트를 제공. 이 이벤트가 발생하면 로컬 캐시로 폴백하고, 요청정보에 대한 복구가 완료되면 다시 redis를 활용하도록 구성
